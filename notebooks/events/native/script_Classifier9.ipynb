{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import re\n",
    "import graphlab as gl\n",
    "from graphlab.toolkits.feature_engineering import TFIDF, FeatureHasher, QuadraticFeatures\n",
    "\n",
    "gl.canvas.set_target('ipynb')\n",
    "\n",
    "PATH_TO_JSON2 = \"/mnt/sframe/docs_prod_02/\"\n",
    "PATH_TO_JSON = \"/mnt/sframe/docs_prod_05/\"\n",
    "PATH_TO_JSON6 = \"/mnt/sframe/docs_prod_06/\"\n",
    "PATH_TO_TRAIN_LABELS = \"input/train.csv\"\n",
    "PATH_TO_TEST_LABELS = \"input/sampleSubmission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This trial license of GraphLab Create is assigned to renatbek@gmail.com and will expire on October 08, 2015. Please contact trial@dato.com for licensing options or to request a free non-commercial license for personal or academic use.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-11685 - Server binary: /usr/local/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1443941241.log\n",
      "[INFO] GraphLab Server Version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "gl.set_runtime_config('GRAPHLAB_DEFAULT_NUM_PYLAMBDA_WORKERS', 128)\n",
    "gl.set_runtime_config('GRAPHLAB_FILEIO_MAXIMUM_CACHE_CAPACITY', 100*1024*1024*1024) # 100GB\n",
    "gl.set_runtime_config('GRAPHLAB_FILEIO_MAXIMUM_CACHE_CAPACITY_PER_FILE', 100*1024*1024*1024) # 100GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transf(x):\n",
    "    return 50.0 * np.log1p(np.log1p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transf(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gl.get_runtime_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# documents\n",
    "sf = gl.SFrame.read_csv(PATH_TO_JSON, header=False, verbose=False)\n",
    "sf = sf.unpack('X1',column_name_prefix='')\n",
    "sf['id'] = sf['id'].apply(lambda x: str(x.split('_')[0] ))\n",
    "sf['num_words'] = sf['text'].apply(lambda xs: transf(len(xs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf2 = gl.SFrame.read_csv(PATH_TO_JSON2, header=False, verbose=False)\n",
    "sf2 = sf2.unpack('X1',column_name_prefix='')\n",
    "sf2['id'] = sf2['id'].apply(lambda x: str(x.split('_')[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf6 = gl.SFrame.read_csv(PATH_TO_JSON6, header=False, verbose=False)\n",
    "sf6 = sf6.unpack('X1',column_name_prefix='')\n",
    "sf6['id'] = sf6['id'].apply(lambda x: str(x.split('_')[0] ))\n",
    "sf6['word2vec'] = sf6['word2vec'].apply(lambda xs: np.array(xs ,dtype='float32').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_cnt = gl.SFrame()\n",
    "sf_cnt['id'] = sf2['id']\n",
    "\n",
    "sf_cnt['a_href'] = sf2['ahref'].apply(lambda x: transf(len(x)))\n",
    "sf_cnt['par'] = sf2['par'].apply(lambda x: transf(len(x)))\n",
    "sf_cnt['title'] = sf2['title'].apply(lambda x: transf(len(x)))\n",
    "\n",
    "sf_cnt['img'] = sf2['img_cnt'].apply(transf)\n",
    "sf_cnt['btn'] = sf2['misc_button'].apply(transf)\n",
    "sf_cnt['input'] = sf2['misc_input'].apply(transf)\n",
    "sf_cnt['li'] = sf2['misc_li'].apply(transf)\n",
    "sf_cnt['link'] = sf2['misc_link'].apply(transf)\n",
    "sf_cnt['meta'] = sf2['misc_meta'].apply(transf)\n",
    "\n",
    "sf_cnt['script_avg'] = sf2['script_avg'].apply(transf)\n",
    "sf_cnt['script_b_avg'] = sf2['script_b_avg'].apply(transf)\n",
    "sf_cnt['script_cnt'] = sf2['script_cnt'].apply(transf)\n",
    "sf_cnt['script_b_cnt'] = sf2['script_b_cnt'].apply(transf)\n",
    "\n",
    "sf_cnt['style_avg'] = sf2['style_avg'].apply(transf)\n",
    "sf_cnt['style_cnt'] = sf2['style_cnt'].apply(transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train/test labels and merge into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train/test labels\n",
    "train_labels = gl.SFrame.read_csv(PATH_TO_TRAIN_LABELS, verbose=False)\n",
    "test_labels = gl.SFrame.read_csv(PATH_TO_TEST_LABELS, verbose=False)\n",
    "train_labels['id'] = train_labels['file'].apply(lambda x: str(x.split('_')[0] ))\n",
    "train_labels = train_labels.remove_column('file')\n",
    "test_labels['id'] = test_labels['file'].apply(lambda x: str(x.split('_')[0] ))\n",
    "test_labels = test_labels.remove_column('file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join\n",
    "train = train_labels.join(sf, on='id', how='left')\n",
    "test = test_labels.join(sf, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.join(sf_cnt, on='id', how='left')\n",
    "test = test.join(sf_cnt, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.join(sf6, on='id', how='left')\n",
    "test = test.join(sf6, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "            'a_href',\n",
    "            'par',\n",
    "            'title',\n",
    "            'img',\n",
    "            'btn',\n",
    "            'input',\n",
    "            'li',\n",
    "            'link',\n",
    "            'meta',\n",
    "            'script_avg',\n",
    "            'script_b_avg',\n",
    "            'script_cnt',\n",
    "            'script_b_cnt',\n",
    "            'style_avg',\n",
    "            'style_cnt',\n",
    "            'num_words'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill in empty\n",
    "for f in features:\n",
    "    train = train.fillna(f, 0.0)     \n",
    "    test = test.fillna(f, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna('shinn', {})     \n",
    "test = test.fillna('shinn', {})\n",
    "\n",
    "train['shinn'] = train['shinn'].apply(lambda ws: ws if ws else {})\n",
    "test['shinn'] = test['shinn'].apply(lambda ws: ws if ws else {})\n",
    "\n",
    "features = features + ['shinn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna('word2vec', np.zeros(300))     \n",
    "test = test.fillna('word2vec', np.zeros(300))\n",
    "\n",
    "train['word2vec'] = train['word2vec'].apply(lambda ws: ws if ws else np.zeros(300))\n",
    "test['word2vec'] = test['word2vec'].apply(lambda ws: ws if ws else np.zeros(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna('words', [])     \n",
    "test = test.fillna('words', [])   \n",
    "\n",
    "train['words'] = train['words'].apply(lambda ws: ws if ws else [])\n",
    "test['words'] = test['words'].apply(lambda ws: ws if ws else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['word_set_size'] = train['words'].apply(lambda ws: len(set(ws)))\n",
    "test['word_set_size'] = test['words'].apply(lambda ws: len(set(ws)))\n",
    "\n",
    "train['word_set_size_ratio'] = train.apply(lambda r: r['word_set_size'] * 1.0 / len(r['words']) if len(r['words']) > 0 else 0.0)\n",
    "test['word_set_size_ratio'] = test.apply(lambda r: r['word_set_size'] * 1.0 / len(r['words']) if len(r['words']) > 0 else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['text_words'] = train['words'].apply(lambda ws: ' '.join(ws))\n",
    "test['text_words'] = test['words'].apply(lambda ws: ' '.join(ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = features + ['word_set_size', 'word_set_size_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_trn = gl.text_analytics.count_words(train['text_words'])\n",
    "bow_trn = bow_trn.dict_trim_by_keys(gl.text_analytics.stopwords())\n",
    "\n",
    "bow_tst = gl.text_analytics.count_words(test['text_words'])\n",
    "bow_tst = bow_tst.dict_trim_by_keys(gl.text_analytics.stopwords())\n",
    "\n",
    "train['bow_words'] = bow_trn\n",
    "test['bow_words'] = bow_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_trn = gl.text_analytics.count_words(train['text'])\n",
    "bow_trn = bow_trn.dict_trim_by_keys(gl.text_analytics.stopwords())\n",
    "\n",
    "bow_tst = gl.text_analytics.count_words(test['text'])\n",
    "bow_tst = bow_tst.dict_trim_by_keys(gl.text_analytics.stopwords())\n",
    "\n",
    "train['bow'] = bow_trn\n",
    "test['bow'] = bow_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = gl.feature_engineering.create(train, TFIDF('bow_words', output_column_name='tfidf_words', min_document_frequency=5e-5))\n",
    "train = encoder.transform(train)\n",
    "test = encoder.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = gl.feature_engineering.create(train, TFIDF('bow', output_column_name='tfidf', min_document_frequency=5e-5))\n",
    "train = encoder.transform(train)\n",
    "test = encoder.transform(test)\n",
    "\n",
    "train_train['tfidf'] = train_train['tfidf'].fillna({})\n",
    "train_cv['tfidf'] = train_cv['tfidf'].fillna({})\n",
    "\n",
    "train_train['tfidf'] = train_train['tfidf'].apply(lambda x: x if x else {})\n",
    "train_cv['tfidf'] = train_cv['tfidf'].apply(lambda x: x if x else {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash_encoder = gl.feature_engineering.create(train, FeatureHasher(features = ['tfidf'], num_bits=18, \n",
    "                                                                  output_column_name='tfidf_hashed_18'))\n",
    "train['tfidf_hashed_18'] = hash_encoder.transform(train)['tfidf_hashed_18']\n",
    "test['tfidf_hashed_18'] = hash_encoder.transform(test)['tfidf_hashed_18']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training set for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_train, train_cv = train.random_split(0.80, seed=107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN, CV = train_cv.random_split(0.50, seed=113)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Submission Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started at 20:36:00am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gl.classifier.random_forest_classifier.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model200=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_imp = model200.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gl.canvas.set_target('ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fi = list(gl.load_sframe('feature_importance.csv')['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode_dict(a):\n",
    "    #a = \"shinn[\\\"hello\\\"]\"\n",
    "    dic, word = a.split('[')\n",
    "    _, word, _ = word.split('\\\"')\n",
    "    return dic, word\n",
    "\n",
    "def get_or_else(dic, word, no=0.0):\n",
    "    if dic.has_key(word) and dic[word]:\n",
    "        return dic[word]\n",
    "    else:\n",
    "        return no\n",
    "    \n",
    "def value_it(a, data, out):\n",
    "    if '[' in a:\n",
    "        dic, word = decode_dict(a)\n",
    "        out[dic + '.' + word] = data[dic].apply(lambda d: get_or_else(d, word))\n",
    "    else:\n",
    "        out[a] = data[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_l = gl.SFrame()\n",
    "for a in fi[0:150]:\n",
    "    value_it(a, CV, CV_l)\n",
    "    \n",
    "TRAIN_l = gl.SFrame()\n",
    "for a in fi[0:150]:\n",
    "    value_it(a, TRAIN, TRAIN_l)    \n",
    "    \n",
    "CV_l['sponsored'] = CV['sponsored']\n",
    "TRAIN_l['sponsored'] = TRAIN['sponsored']\n",
    "new_feats = set(CV_l.column_names()).difference(set(['sponsored']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'sponsored' in new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = {}\n",
    "get_or_else(a, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_l = TRAIN.unpack('shinn')\n",
    "CV_l = CV.unpack('shinn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_l = CV_l.unpack('tfidf5e5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'sponsored' in set(CV_l.column_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_l['tfidf5e5.copyright'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV[CV['sponsored'] == 0]['a_href'].sum()  / CV[CV['sponsored'] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gl.classifier.boosted_trees_classifier.create(train, target='sponsored',\n",
    "                                                      #features=features + ['tfidf_hashed_18'],\n",
    "                                                      features=features + ['tfidf5e5'],\n",
    "                                                      max_depth=6,\n",
    "                                                      step_size=0.2,\n",
    "                                                      max_iterations=300,\n",
    "                                                      column_subsample=0.3,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gl.classifier.random_forest_classifier.create(train, target='sponsored',\n",
    "                                                      features=features + ['tfidf', 'word2vec'],\n",
    "                                                      num_trees=200,\n",
    "                                                      max_depth=150,\n",
    "                                                      validation_set=None,\n",
    "                                                      column_subsample=0.45,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gl.classifier.boosted_trees_classifier.create(train, target='sponsored',\n",
    "                                                      #features=features + ['tfidf_hashed_18'],\n",
    "                                                      features=features + ['tfidf'],\n",
    "                                                      max_depth=6,\n",
    "                                                      step_size=0.2,\n",
    "                                                      max_iterations=300,\n",
    "                                                      column_subsample=0.3,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto',\n",
    "                                                      validation_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = gl.logistic_classifier.create(TRAIN_l, target='sponsored', \n",
    "                                      features=new_feats,\n",
    "                                      validation_set=CV_l,\n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=10,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model = gl.svm_classifier.create(train, target='sponsored', \n",
    "                                      features=['tfidf_hashed'],\n",
    "                                      validation_set=None,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(test, 'probability')\n",
    "\n",
    "submission = gl.SFrame()\n",
    "submission['file'] = test['id'].apply(lambda x: x + '_raw_html.txt')\n",
    "submission['sponsored'] = ypred \n",
    "#submission.save('submission_version_4.csv', format='csv')\n",
    "\n",
    "submission = submission.to_dataframe()\n",
    "submission.to_csv('submission_rf_word_set_word2vec.csv', index=False, float_format='%1.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('/mnt/sframe/model_RF_200_150_noword2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train into *train_train*/*train_cv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_train = train_train.dropna()\n",
    "train_cv = train_cv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=features + ['tfidf'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=30,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = gl.SFrame()\n",
    "results['id'] = train_cv['id']\n",
    "results['actual'] = train_cv['sponsored']\n",
    "results['predicted'] = model.predict(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cv.unpack('tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FN.shape, FP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FN = results[results['actual'] > results['predicted']]\n",
    "FP = results[results['actual'] < results['predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FN[720:730]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cv.remove_column('tfidf_hashed17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model = gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=features + ['tfidf_hashed'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cv['margin'] = svm_model.predict(train_cv, output_type='margin')\n",
    "preds = train_cv[['sponsored', 'margin']].sort('margin')\n",
    "train_cv.remove_column('margin')\n",
    "\n",
    "pd_preds = preds.to_dataframe()\n",
    "pd_preds['number'] = 1.0\n",
    "\n",
    "pd_preds_cum = pd_preds.cumsum()\n",
    "\n",
    "total_positives = np.asarray(pd_preds_cum['sponsored'])[-1]\n",
    "total = np.asarray(pd_preds_cum['number'])[-1]\n",
    "total_negatives = total - total_positives\n",
    "\n",
    "pd_preds_cum['FN'] = pd_preds_cum['sponsored']\n",
    "pd_preds_cum['TN'] = pd_preds_cum['number'] - pd_preds_cum['sponsored']\n",
    "\n",
    "pd_preds_cum['TP'] = total_positives - pd_preds_cum['FN']\n",
    "pd_preds_cum['FP'] = total - total_positives - pd_preds_cum['TN']\n",
    "\n",
    "pd_preds_cum['fpr'] = pd_preds_cum['FP'] / (pd_preds_cum['FP'] + pd_preds_cum['TN'])\n",
    "pd_preds_cum['tpr'] = pd_preds_cum['TP'] / (pd_preds_cum['TP'] + pd_preds_cum['FN'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd_preds_cum\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cv['margin'] = svm_model.predict(train_cv, output_type='margin')\n",
    "preds = train_cv[['sponsored', 'margin']]\n",
    "preds['margin'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(preds[preds['margin'] < 55]['sponsored']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts = np.arange(-22, 50, 0.1)\n",
    "[for t in ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = svm_model.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_train = gl.load_sframe('/mnt/sframe/shinn_split_train_train')\n",
    "train_cv = gl.load_sframe('/mnt/sframe/shinn_split_train_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_encoder = gl.feature_engineering.create(train_train, FeatureHasher(features = ['tfidf'], num_bits=17, \n",
    "                                                                  output_column_name='tfidf_hashed_17'))\n",
    "train_train['tfidf_hashed_17'] = hash_encoder.transform(train_train)['tfidf_hashed_17']\n",
    "train_cv['tfidf_hashed_17'] = hash_encoder.transform(train_cv)['tfidf_hashed_17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_15 = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_15'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_15.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_15.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_16 = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_16'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=20,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_16.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_16.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_17 = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_17'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights=None, #'auto',\n",
    "                                      max_iterations=8,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_17 = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_17'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights=None, #'auto',\n",
    "                                      max_iterations=7,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_17.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_17.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_15'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_16'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_17'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = gl.load_sframe('/mnt/sframe/counts_and_tfidf_hashed_18_train')\n",
    "test = gl.load_sframe('/mnt/sframe/counts_and_tfidf_hashed_18_test')\n",
    "\n",
    "train_train = gl.load_sframe('/mnt/sframe/num_words_counts_and_tfidf_hashed_18_split_train_train')\n",
    "train_cv = gl.load_sframe('/mnt/sframe/num_words_counts_and_tfidf_hashed_18_split_train_cv')\n",
    "\n",
    "TRAIN, CV = train_cv.random_split(0.50, seed=113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.save('/mnt/sframe/shinn_train')\n",
    "test.save('/mnt/sframe/shinn_test')\n",
    "\n",
    "train_train.save('/mnt/sframe/shinn_split_train_train')\n",
    "train_cv.save('/mnt/sframe/shinn_split_train_cv')\n",
    "#train_train = gl.load_sframe('/mnt/sframe/tfidf_hashed_16_split_train_train')\n",
    "#train_cv = gl.load_sframe('/mnt/sframe/tfidf_hashed_16_split_train_cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model2 = gl.classifier.random_forest_classifier.create(train_train, target='sponsored',\n",
    "#                                                       features=features + ['word2vec'],\n",
    "#                                                       num_trees=10,\n",
    "#                                                       max_depth=200,\n",
    "#                                                       column_subsample=0.15,\n",
    "#                                                       row_subsample=1.0,\n",
    "#                                                       class_weights='auto',\n",
    "#                                                       validation_set=train_cv)\n",
    "model_boosted = gl.classifier.boosted_trees_classifier.create(train_train, target='sponsored',\n",
    "                                                      features=features + ['word2vec'],\n",
    "                                                      max_depth=6,\n",
    "                                                      step_size=1.0,  #0.2\n",
    "                                                      max_iterations=300,\n",
    "                                                      column_subsample=0.25,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto',\n",
    "                                                      validation_set=train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_boosted.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1_word2vec = gl.classifier.random_forest_classifier.create(train_train, target='sponsored',\n",
    "                                                      features=features + ['tfidf', 'word2vec'],\n",
    "                                                      num_trees=90, #100,\n",
    "                                                      max_depth=150,\n",
    "                                                      column_subsample=0.45,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto',\n",
    "                                                      validation_set=train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model1_word2vec.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = gl.classifier.random_forest_classifier.create(train_train, target='sponsored',\n",
    "                                                      features=features + ['tfidf'], #, 'word2vec'],\n",
    "                                                      num_trees=90, #100,\n",
    "                                                      max_depth=150,\n",
    "                                                      column_subsample=0.45,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto',\n",
    "                                                      validation_set=train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model1.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = gl.classifier.random_forest_classifier.create(train_train, target='sponsored',\n",
    "                                                      features=features + ['tfidf'], #, 'word2vec'],\n",
    "                                                      num_trees=10,\n",
    "                                                      max_depth=150,\n",
    "                                                      column_subsample=0.45,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto',\n",
    "                                                      validation_set=train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.get_feature_importance().print_rows(num_rows=30, num_columns=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_train['tfidf_nonone'] = train_train['tfidf'].apply(lambda x: x if x else {})\n",
    "train_cv['tfidf_nonone'] = train_cv['tfidf'].apply(lambda x: x if x else {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_train['tfidf_nonone'] = train_train['tfidf_nonone'].fillna({})\n",
    "train_cv['tfidf_nonone'] = train_cv['tfidf_nonone'].fillna({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model = gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=features + ['tfidf_nonone'], #features + ['tfidf'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cv['margin'] = svm_model.predict(train_cv, output_type='margin')\n",
    "preds = train_cv[['sponsored', 'margin']].sort('margin')\n",
    "train_cv.remove_column('margin')\n",
    "\n",
    "pd_preds = preds.to_dataframe()\n",
    "pd_preds['number'] = 1.0\n",
    "\n",
    "pd_preds_cum = pd_preds.cumsum()\n",
    "\n",
    "total_positives = np.asarray(pd_preds_cum['sponsored'])[-1]\n",
    "total = np.asarray(pd_preds_cum['number'])[-1]\n",
    "total_negatives = total - total_positives\n",
    "\n",
    "pd_preds_cum['FN'] = pd_preds_cum['sponsored']\n",
    "pd_preds_cum['TN'] = pd_preds_cum['number'] - pd_preds_cum['sponsored']\n",
    "\n",
    "pd_preds_cum['TP'] = total_positives - pd_preds_cum['FN']\n",
    "pd_preds_cum['FP'] = total - total_positives - pd_preds_cum['TN']\n",
    "\n",
    "pd_preds_cum['fpr'] = pd_preds_cum['FP'] / (pd_preds_cum['FP'] + pd_preds_cum['TN'])\n",
    "pd_preds_cum['tpr'] = pd_preds_cum['TP'] / (pd_preds_cum['TP'] + pd_preds_cum['FN'])\n",
    "\n",
    "#   show\n",
    "a = pd_preds_cum\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_boosted = gl.classifier.boosted_trees_classifier.create(train_train, target='sponsored',\n",
    "                                                      features=features + ['tfidf_hashed_18'],\n",
    "                                                      max_depth=6,\n",
    "                                                      step_size=0.2,\n",
    "                                                      max_iterations=300,\n",
    "                                                      column_subsample=0.3,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto',\n",
    "                                                      validation_set=train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_boosted.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.boosted_trees_classifier.get_default_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN, CV = train_cv.random_split(0.50, seed=113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_boosted = gl.classifier.boosted_trees_classifier.create(TRAIN, target='sponsored',\n",
    "                                                      features=features + ['tfidf_hashed_18'],\n",
    "                                                      max_depth=6,\n",
    "                                                      step_size=0.2,\n",
    "                                                      max_iterations=500,\n",
    "                                                      column_subsample=0.3,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto',\n",
    "                                                      validation_set=CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_boosted.evaluate(CV, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_boosted = gl.classifier.boosted_trees_classifier.create(train_train, target='sponsored',\n",
    "                                                      features=features + ['tfidf_hashed_18'],\n",
    "                                                      max_depth=6,\n",
    "                                                      step_size=0.2,\n",
    "                                                      max_iterations=400,\n",
    "                                                      column_subsample=0.4,\n",
    "                                                      row_subsample=1.0,\n",
    "                                                      class_weights='auto',\n",
    "                                                      validation_set=train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cv_pred = gl.SFrame()\n",
    "train_cv_pred['pred'] =model_boosted.predict(train_cv)\n",
    "train_cv_pred['actual'] = train_cv['sponsored']\n",
    "train_cv_pred['id'] = train_cv['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_over = train_cv_pred[train_cv_pred['pred'] > train_cv_pred['actual']]\n",
    "pred_under = train_cv_pred[train_cv_pred['pred'] < train_cv_pred['actual']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for z in list(pred_under.sample(0.02)['id'].apply(lambda x: 'aws s3 cp s3://sparkydotsdata/kaggle/native/orig/' + x + '_raw_html.txt ' + x +'raw_html')):\n",
    "    print(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_boosted.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_evaluator(model, train, test):\n",
    "    results = model.evaluate(test, metric='roc_curve')\n",
    "    a = results['roc_curve']\n",
    "\n",
    "    fpr = list(a['fpr'])\n",
    "    tpr = list(a['tpr'])\n",
    "    fpr[0] = 1.0\n",
    "    tpr[0] = 1.0\n",
    "    fpr = np.array(fpr)\n",
    "    tpr = np.array(tpr)\n",
    "\n",
    "    AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "    return {'AUC': AUC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job0 = job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = dict([\n",
    "        ('target', 'sponsored'),\n",
    "        ('features', [features + ['tfidf_hashed_18']]),\n",
    "        ('max_depth', [6]),\n",
    "        ('step_size', [0.2]),\n",
    "        ('max_iterations', [100, 150, 200]),\n",
    "        ('column_subsample', [0.4]),\n",
    "        ('validation_set', [None])\n",
    "    ])\n",
    "\n",
    "job = gl.grid_search.create((TRAIN, CV), \n",
    "                              gl.boosted_trees_classifier.create, \n",
    "                              params, \n",
    "                              evaluator=custom_evaluator)\n",
    "job.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = job.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = job.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.boxplot('AUC', by='max_iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = scipy.stats.distributions.expon(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=0)\n",
    "#train = df_full[df_full.sponsored.notnull()].fillna(0)\n",
    "#test = df_full[df_full.sponsored.isnull() & df_full.file.isin(test_files)].fillna(0)\n",
    "# clf.fit(train.drop(['file', 'sponsored'], 1), train.sponsored)\n",
    "\n",
    "# print('--- Create predictions and submission')\n",
    "# submission = test[['file']].reset_index(drop=True)\n",
    "# submission['sponsored'] = clf.predict_proba(test.drop(['file', 'sponsored'], 1))[:, 1]\n",
    "# submission.to_csv('native_btb_basic_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiTRAIN = shiTRAIN.to_dataframe()\n",
    "shiCV = shiCV.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in shiTRAIN.column_names:\n",
    "    shiTRAIN[col] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(shiTRAIN, shiTRAIN_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shiTRAIN = TRAIN.unpack('shinn')\n",
    "shiCV = CV.unpack('shinn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiTRAIN_label = np.asarray(shiTRAIN['sponsored'])\n",
    "shiCV_label = np.asarray(shiCV['sponsored'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shiTRAIN_label = np.asarray(shiTRAIN_label, float)\n",
    "shiCV_label = np.asarray(shiCV_label, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiTRAIN_tf = shiTRAIN['tfidf5e5']\n",
    "shiCV_tf = shiCV['tfidf5e5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shiTRAIN.remove_columns(['tfidf5e5'])\n",
    "shiCV.remove_columns(['tfidf5e5'])\n",
    "# shiTRAIN.remove_columns(['text', 'bow', 'sponsored', 'id'])\n",
    "# shiCV.remove_columns(['text', 'bow', 'sponsored', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shiTRAIN = shiTRAIN.to_dataframe drop('text', 1)\n",
    "shiCV = shiCV.drop('text', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shiTRAIN.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
