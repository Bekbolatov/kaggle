{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] This Python session does not appear to be running in an interactive IPython Notebook. Use of the 'ipynb' target may behave unexpectedly or result in errors.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import graphlab as gl\n",
    "from graphlab.toolkits.feature_engineering import TFIDF, FeatureHasher, QuadraticFeatures\n",
    "\n",
    "gl.canvas.set_target('ipynb')\n",
    "\n",
    "PATH_TO_JSON = \"docs_prod_03/\"\n",
    "PATH_TO_TRAIN_LABELS = \"input/train.csv\"\n",
    "PATH_TO_TEST_LABELS = \"input/sampleSubmission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This trial license of GraphLab Create is assigned to renatbek@gmail.com and will expire on October 08, 2015. Please contact trial@dato.com for licensing options or to request a free non-commercial license for personal or academic use.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-13879 - Server binary: /usr/local/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1442476111.log\n",
      "[INFO] GraphLab Server Version: 1.5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/20\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.1496 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[dict]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/20\n",
      "PROGRESS: Read 5681 lines. Lines per second: 14558.2\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/7\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/24\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/2\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/0\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/54\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/51\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/42\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/67\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/44\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/27\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/16\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/31\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/35\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/69\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/64\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/61\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/57\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/40\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/55\n",
      "PROGRESS: Read 113689 lines. Lines per second: 20667.8\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/53\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/4\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/34\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/41\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/38\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/33\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/28\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/52\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/12\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/13\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/39\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/60\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/8\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/65\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/50\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/1\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/45\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/63\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/3\n",
      "PROGRESS: Read 221721 lines. Lines per second: 20979\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/22\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/49\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/59\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/9\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/68\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/26\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/18\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/29\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/15\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/70\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/58\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/37\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/25\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/5\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/48\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/47\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/32\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/17\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/14\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/66\n",
      "PROGRESS: Read 335384 lines. Lines per second: 21436.3\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/6\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/10\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/62\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/30\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/56\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/21\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/19\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/23\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/46\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/43\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/11\n",
      "PROGRESS: Finished parsing file /mnt/data/docs_prod_03/36\n",
      "PROGRESS: Parsing completed. Parsed 403618 lines in 19.1965 secs.\n"
     ]
    }
   ],
   "source": [
    "# documents\n",
    "sf = gl.SFrame.read_csv(PATH_TO_JSON, header=False, verbose=False)\n",
    "sf = sf.unpack('X1',column_name_prefix='')\n",
    "sf['id'] = sf['id'].apply(lambda x: str(x.split('_')[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train/test labels and merge into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /mnt/data/input/train.csv\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.157487 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /mnt/data/input/train.csv\n",
      "PROGRESS: Parsing completed. Parsed 337024 lines in 0.129273 secs.\n",
      "PROGRESS: Finished parsing file /mnt/data/input/sampleSubmission.csv\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.052125 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /mnt/data/input/sampleSubmission.csv\n",
      "PROGRESS: Parsing completed. Parsed 66772 lines in 0.049413 secs.\n"
     ]
    }
   ],
   "source": [
    "# train/test labels\n",
    "train_labels = gl.SFrame.read_csv(PATH_TO_TRAIN_LABELS, verbose=False)\n",
    "test_labels = gl.SFrame.read_csv(PATH_TO_TEST_LABELS, verbose=False)\n",
    "train_labels['id'] = train_labels['file'].apply(lambda x: str(x.split('_')[0] ))\n",
    "train_labels = train_labels.remove_column('file')\n",
    "test_labels['id'] = test_labels['file'].apply(lambda x: str(x.split('_')[0] ))\n",
    "test_labels = test_labels.remove_column('file')\n",
    "\n",
    "# join\n",
    "train = train_labels.join(sf, on='id', how='left')\n",
    "test = test_labels.join(sf, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_trn = gl.text_analytics.count_words(train['text'])\n",
    "bow_trn = bow_trn.dict_trim_by_keys(gl.text_analytics.stopwords())\n",
    "\n",
    "bow_tst = gl.text_analytics.count_words(test['text'])\n",
    "bow_tst = bow_tst.dict_trim_by_keys(gl.text_analytics.stopwords())\n",
    "\n",
    "train['bow'] = bow_trn\n",
    "test['bow'] = bow_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = gl.feature_engineering.create(train, TFIDF('bow', output_column_name='tfidf', min_document_frequency=1e-5))\n",
    "train = encoder.transform(train)\n",
    "test = encoder.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash_encoder = gl.feature_engineering.create(train, FeatureHasher(features = ['tfidf'], num_bits=16, \n",
    "                                                                  output_column_name='tfidf_hashed'))\n",
    "train['tfidf_hashed'] = hash_encoder.transform(train)['tfidf_hashed']\n",
    "test['tfidf_hashed'] = hash_encoder.transform(test)['tfidf_hashed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gl.logistic_classifier.create(train, target='sponsored', \n",
    "                                      features=['tfidf_hashed'],\n",
    "                                      validation_set=None,\n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=5,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = model.predict(test)\n",
    "\n",
    "submission = gl.SFrame()\n",
    "submission['sponsored'] = ypred \n",
    "submission['file'] = test['id'].apply(lambda x: x + '_raw_html.txt')\n",
    "submission.save('submission_version_4.csv', format='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train into *train_train*/*train_cv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_train, train_cv = train.random_split(0.80, seed=107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=20,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model = gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = svm_model.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_train.save('dataset_train_train')\n",
    "train_cv.save('dataset_train_cv')\n",
    "test.save('dataset_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_encoder = gl.feature_engineering.create(train_train, FeatureHasher(features = ['tfidf'], num_bits=17, \n",
    "                                                                  output_column_name='tfidf_hashed_17'))\n",
    "train_train['tfidf_hashed_17'] = hash_encoder.transform(train_train)['tfidf_hashed_17']\n",
    "train_cv['tfidf_hashed_17'] = hash_encoder.transform(train_cv)['tfidf_hashed_17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_15 = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_15'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_15.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_15.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_16 = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_16'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=20,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_16.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_16.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_17 = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_17'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights=None, #'auto',\n",
    "                                      max_iterations=8,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_17 = gl.logistic_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_17'],\n",
    "                                      validation_set=train_cv,\n",
    "                                      class_weights=None, #'auto',\n",
    "                                      max_iterations=7,\n",
    "                                      feature_rescaling=True,\n",
    "                                      l2_penalty=0.00,\n",
    "                                      l1_penalty=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_17.evaluate(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model_17.evaluate(train_cv, metric='roc_curve')\n",
    "a = results['roc_curve']\n",
    "\n",
    "fpr = list(a['fpr'])\n",
    "tpr = list(a['tpr'])\n",
    "fpr[0] = 1.0\n",
    "tpr[0] = 1.0\n",
    "fpr = np.array(fpr)\n",
    "tpr = np.array(tpr)\n",
    "\n",
    "AUC = np.sum((fpr[:-1] - fpr[1:]) * (tpr[:-1] + (tpr[:-1] - tpr[1:])/2))\n",
    "plt.plot(fpr, tpr)\n",
    "print('AUC = %f'%AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_15'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_16'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.svm_classifier.create(train_train, target='sponsored', \n",
    "                                      features=['tfidf_hashed_17'],\n",
    "                                      validation_set=train_cv,                                           \n",
    "                                      class_weights='auto',\n",
    "                                      max_iterations=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
