{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import re\n",
    "\n",
    "data_loc = '/Users/rbekbolatov/data/kaggle/native/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs = {}\n",
    "\n",
    "for i in range(5):\n",
    "    files = pd.read_csv(data_loc + 'files_in/files_in_' + str(i), header=None, names=['filename'])\n",
    "    files['dir'] = i\n",
    "    dirs[i] = set(files['filename'])\n",
    "    \n",
    "def get_dir(filename):\n",
    "    for i in range(5):\n",
    "        if filename in dirs[i]:\n",
    "            return i\n",
    "    return -1  \n",
    "\n",
    "eval_labels = pd.read_csv(data_loc + 'train.csv')\n",
    "eval_labels_sample = eval_labels.sample(N)\n",
    "\n",
    "train, test = train_test_split(eval_labels_sample, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_files = pd.read_csv(data_loc + 'all_filenames.csv', header=None, names=['file', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_subsamples(all_files):\n",
    "    files_8k = all_files.sample(8000, random_state = 101)\n",
    "    files_3k = all_files.sample(3000, random_state = 101)\n",
    "    files_100 = all_files.sample(100, random_state = 101)\n",
    "    # save files\n",
    "    subsamples_loc = data_loc + 'subsamples/'\n",
    "    files_8k['file'].to_csv(subsamples_loc + 'files8k.csv', index=False)\n",
    "    files_3k['file'].to_csv(subsamples_loc + 'files3k.csv', index=False)\n",
    "    files_100['file'].to_csv(subsamples_loc + 'files100.csv', index=False)\n",
    "    # have filenames here\n",
    "    files_8k = np.asarray(files_8k['file'])\n",
    "    files_3k = np.asarray(files_3k['file'])\n",
    "    files_100 = np.asarray(files_100['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_soup(filename):\n",
    "    file_handle = open(data_loc + str(get_dir(filename)) + '/' + filename)\n",
    "    file_content = file_handle.read()\n",
    "    file_handle.close()\n",
    "    soup = bs(file_content)\n",
    "    return soup\n",
    "\n",
    "def get_paragraphs(soup):\n",
    "    paragraphs = soup.findAll('p')\n",
    "    cleaned_texts = [re.sub(r'[\\'\"|\\n\\t,.:;()\\-\\/]+', ' ', p.text.encode('ascii', 'ignore').strip()) for p in paragraphs]\n",
    "    return cleaned_texts \n",
    "\n",
    "def get_title(soup):\n",
    "    title = soup.find('title')\n",
    "    title = '' if not title else title.text\n",
    "    title = 'long_title' if len(title) > 200 else title.encode('ascii', 'ignore').strip().replace('\\n',' ')\n",
    "    return title\n",
    "\n",
    "def get_links(soup):\n",
    "    hrefs = []\n",
    "    texts = []\n",
    "    links = soup.findAll('a')\n",
    "    hrefs = [a.href for a in links]\n",
    "    texts = [re.sub(r'[\\'\"|\\n\\t,.:;()\\-\\/]+', ' ', a.text.encode('ascii', 'ignore').strip()) for a in links]\n",
    "    return hrefs, texts\n",
    "    \n",
    "def get_tag_data(files):\n",
    "    data = []\n",
    "    for filename in files:\n",
    "        with open(data_loc + str(get_dir(filename)) + '/' + filename, 'r') as f:\n",
    "            file_content = f.read()\n",
    "            #print (file_content)\n",
    "            if file_content:\n",
    "                soup = bs(file_content, 'lxml') #, 'html.parser')\n",
    "                title = get_title(soup)\n",
    "                link_hrefs, link_texts = get_links(soup)\n",
    "                data.append((filename, title, len(link_hrefs)))\n",
    "\n",
    "    print 'Data size: %d' % len(data)\n",
    "    data = pd.DataFrame(data, columns = ['file', 'title', 'num_a'])\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 100\n",
      "Data size: 100\n"
     ]
    }
   ],
   "source": [
    "data_train = get_tag_data(train['file'])\n",
    "data_test = get_tag_data(test['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train = train.merge(data_train)\n",
    "data_test = test.merge(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>sponsored</th>\n",
       "      <th>title</th>\n",
       "      <th>num_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2294986_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Penny Arcade - Comic - My Comeuppance</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3797287_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedian Interviews, Funny Articles, Jokes &amp; M...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3953071_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Rockout with Rocksmith 2014 Edition - I Wanna ...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2864376_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Easy recipe: Chocolate cream</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2905436_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Creative Compulsive Disorder: Remembering Zina...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file  sponsored  \\\n",
       "10  2294986_raw_html.txt          0   \n",
       "11  3797287_raw_html.txt          0   \n",
       "12  3953071_raw_html.txt          0   \n",
       "13  2864376_raw_html.txt          0   \n",
       "14  2905436_raw_html.txt          0   \n",
       "\n",
       "                                                title  num_a  \n",
       "10              Penny Arcade - Comic - My Comeuppance     94  \n",
       "11  Comedian Interviews, Funny Articles, Jokes & M...     92  \n",
       "12  Rockout with Rocksmith 2014 Edition - I Wanna ...    126  \n",
       "13                       Easy recipe: Chocolate cream    145  \n",
       "14  Creative Compulsive Disorder: Remembering Zina...     96  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_labels = pd.read_csv(data_loc + 'train.csv')\n",
    "lb_labels = pd.read_csv(data_loc + 'sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filenames = ['1767762_raw_html.txt', '1542621_raw_html.txt', '625398_raw_html.txt', '1554226_raw_html.txt']\n",
    "filenames = ['625398_raw_html.txt', '1554226_raw_html.txt']\n",
    "soups = [(filename, get_soup(filename)) for filename in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = [soup.find('title').text.encode('ascii', 'ignore').strip() for filename, soup in soups]\n",
    "tags = ['a', 'p', 'div', 'script', 'img', 'ul', 'ol', 'hr', 'b', 'i']\n",
    "tag_data = {filename: {tag: soup.findAll(tag) for tag in tags} for filename, soup in soups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('625398_raw_html.txt',\n",
       "  {'a': 20,\n",
       "   'b': 0,\n",
       "   'div': 40,\n",
       "   'hr': 0,\n",
       "   'i': 0,\n",
       "   'img': 14,\n",
       "   'ol': 0,\n",
       "   'p': 40,\n",
       "   'script': 11,\n",
       "   'ul': 2}),\n",
       " ('1554226_raw_html.txt',\n",
       "  {'a': 358,\n",
       "   'b': 5,\n",
       "   'div': 213,\n",
       "   'hr': 0,\n",
       "   'i': 0,\n",
       "   'img': 209,\n",
       "   'ol': 0,\n",
       "   'p': 28,\n",
       "   'script': 95,\n",
       "   'ul': 32})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(filename, {tag: len(file_tags[tag]) for tag in tags}) for filename, file_tags in tag_data.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['',\n",
       "  'We are surrounded by a society which seems to enjoy inflicting us with their scary birth stories ',\n",
       "  '',\n",
       "  'I have created this part of my website to give you a place to read POSITIVE  encouraging  uplifting birth stories  The majority of the stories are of moms using hypnosis  most of them using Hypnobabies during their births  If you enjoy birth stories  sign up for my newsletter  many will include a positive birth story!',\n",
       "  '',\n",
       "  '',\n",
       "  'Proudly powered by WordPress                                              WordPress Theme Custom Community 2                      developed by ThemeKraft'],\n",
       " ['Pregnancy Birth and Babies',\n",
       "  'Welcome',\n",
       "  'Big Baby Bull',\n",
       "  'Hypnosis for Birth',\n",
       "  'What is Hypnosis for Birth?',\n",
       "  'Comparison of Hypnobabies and HypnoBirthing',\n",
       "  'What are my Options?',\n",
       "  'Hypnosis for Pregnancy',\n",
       "  'Epidural or Hypnobabies or Both?',\n",
       "  'Calmer Baby?',\n",
       "  'Birth Videos',\n",
       "  'Positive Birth Stories',\n",
       "  'VBAC Support',\n",
       "  'Essential Oils',\n",
       "  'Free Book',\n",
       "  '',\n",
       "  'www hypnobabies wordpress com',\n",
       "  'Newsletter',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Amazon com Widgets',\n",
       "  '',\n",
       "  '',\n",
       "  'Visit Sheridan Ripley s profile on Pinterest ',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'By PoseLab',\n",
       "  'Show more videos',\n",
       "  'Proudly powered by WordPress',\n",
       "  'WordPress Theme Custom Community 2'],\n",
       " [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = get_soup('1542621_raw_html.txt') # '1767762_raw_html.txt')\n",
    "texts = get_paragraphs(soup)\n",
    "ts, rs = get_links(soup)\n",
    "texts,rs,ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
