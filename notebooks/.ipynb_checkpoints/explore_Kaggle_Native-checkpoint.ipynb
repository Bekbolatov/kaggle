{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import re\n",
    "\n",
    "data_loc = '/Users/rbekbolatov/data/kaggle/native/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs = {}\n",
    "\n",
    "for i in range(5):\n",
    "    files = pd.read_csv(data_loc + 'files_in/files_in_' + str(i), header=None, names=['filename'])\n",
    "    files['dir'] = i\n",
    "    dirs[i] = set(files['filename'])\n",
    "    \n",
    "def get_dir(filename):\n",
    "    for i in range(5):\n",
    "        if filename in dirs[i]:\n",
    "            return i\n",
    "    return -1  \n",
    "\n",
    "eval_labels = pd.read_csv(data_loc + 'train.csv')\n",
    "eval_labels_sample = eval_labels.sample(N)\n",
    "\n",
    "train, test = train_test_split(eval_labels_sample, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_files = pd.read_csv(data_loc + 'all_filenames.csv', header=None, names=['file', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_subsamples(all_files):\n",
    "    files_8k = all_files.sample(8000, random_state = 101)\n",
    "    files_3k = all_files.sample(3000, random_state = 101)\n",
    "    files_100 = all_files.sample(100, random_state = 101)\n",
    "    # save files\n",
    "    subsamples_loc = data_loc + 'subsamples/'\n",
    "    files_8k['file'].to_csv(subsamples_loc + 'files8k.csv', index=False)\n",
    "    files_3k['file'].to_csv(subsamples_loc + 'files3k.csv', index=False)\n",
    "    files_100['file'].to_csv(subsamples_loc + 'files100.csv', index=False)\n",
    "    # have filenames here\n",
    "    files_8k = np.asarray(files_8k['file'])\n",
    "    files_3k = np.asarray(files_3k['file'])\n",
    "    files_100 = np.asarray(files_100['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    title = soup.find('title')\n",
    "    title = '' if not title else title.text\n",
    "    title = 'long_title' if len(title) > 200 else title.encode('ascii', 'ignore').strip().replace('\\n',' ')\n",
    "    return title\n",
    "\n",
    "def get_links(soup):\n",
    "    hrefs = []\n",
    "    texts = []\n",
    "    links = soup.findAll('a')\n",
    "    hrefs = [a.href for a in links]\n",
    "    texts = [a.text for a in links]\n",
    "    return hrefs, texts\n",
    "    \n",
    "def get_tag_data(files):\n",
    "    data = []\n",
    "    for filename in files:\n",
    "        with open(data_loc + str(get_dir(filename)) + '/' + filename, 'r') as f:\n",
    "            file_content = f.read()\n",
    "            #print (file_content)\n",
    "            if file_content:\n",
    "                soup = bs(file_content, 'lxml') #, 'html.parser')\n",
    "                title = get_title(soup)\n",
    "                link_hrefs, link_texts = get_links(soup)\n",
    "                data.append((filename, title, len(link_hrefs)))\n",
    "\n",
    "    print 'Data size: %d' % len(data)\n",
    "    data = pd.DataFrame(data, columns = ['file', 'title', 'num_a'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 100\n",
      "Data size: 100\n"
     ]
    }
   ],
   "source": [
    "data_train = get_tag_data(train['file'])\n",
    "data_test = get_tag_data(test['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train = train.merge(data_train)\n",
    "data_test = test.merge(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>sponsored</th>\n",
       "      <th>title</th>\n",
       "      <th>num_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2294986_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Penny Arcade - Comic - My Comeuppance</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3797287_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedian Interviews, Funny Articles, Jokes &amp; M...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3953071_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Rockout with Rocksmith 2014 Edition - I Wanna ...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2864376_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Easy recipe: Chocolate cream</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2905436_raw_html.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Creative Compulsive Disorder: Remembering Zina...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file  sponsored  \\\n",
       "10  2294986_raw_html.txt          0   \n",
       "11  3797287_raw_html.txt          0   \n",
       "12  3953071_raw_html.txt          0   \n",
       "13  2864376_raw_html.txt          0   \n",
       "14  2905436_raw_html.txt          0   \n",
       "\n",
       "                                                title  num_a  \n",
       "10              Penny Arcade - Comic - My Comeuppance     94  \n",
       "11  Comedian Interviews, Funny Articles, Jokes & M...     92  \n",
       "12  Rockout with Rocksmith 2014 Edition - I Wanna ...    126  \n",
       "13                       Easy recipe: Chocolate cream    145  \n",
       "14  Creative Compulsive Disorder: Remembering Zina...     96  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello!! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_dir('999174_raw_html.txt'), get_dir('999958_raw_html.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_labels = pd.read_csv(data_loc + 'train.csv')\n",
    "lb_labels = pd.read_csv(data_loc + 'sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_labels_sponsored = eval_labels[eval_labels['sponsored'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>sponsored</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1897377_raw_html.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1767762_raw_html.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file  sponsored\n",
       "12  1897377_raw_html.txt          1\n",
       "39  1767762_raw_html.txt          1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_labels_sponsored[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filenames = ['1767762_raw_html.txt', '1542621_raw_html.txt', '625398_raw_html.txt', '1554226_raw_html.txt']\n",
    "filenames = ['625398_raw_html.txt', '1554226_raw_html.txt']\n",
    "def get_soup(filename):\n",
    "    file_handle = open(data_loc + str(get_dir(filename)) + '/' + filename)\n",
    "    file_content = file_handle.read()\n",
    "    file_handle.close()\n",
    "    soup = bs(file_content)\n",
    "    return soup\n",
    "\n",
    "soups = [(filename, get_soup(filename)) for filename in filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = [soup.find('title').text.encode('ascii', 'ignore').strip() for filename, soup in soups]\n",
    "tags = ['a', 'p', 'div', 'script', 'img', 'ul', 'ol', 'hr', 'b', 'i']\n",
    "tag_data = {filename: {tag: soup.findAll(tag) for tag in tags} for filename, soup in soups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('625398_raw_html.txt',\n",
       "  {'a': 20,\n",
       "   'b': 0,\n",
       "   'div': 40,\n",
       "   'hr': 0,\n",
       "   'i': 0,\n",
       "   'img': 14,\n",
       "   'ol': 0,\n",
       "   'p': 40,\n",
       "   'script': 11,\n",
       "   'ul': 2}),\n",
       " ('1554226_raw_html.txt',\n",
       "  {'a': 358,\n",
       "   'b': 5,\n",
       "   'div': 213,\n",
       "   'hr': 0,\n",
       "   'i': 0,\n",
       "   'img': 209,\n",
       "   'ol': 0,\n",
       "   'p': 28,\n",
       "   'script': 95,\n",
       "   'ul': 32})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(filename, {tag: len(file_tags[tag]) for tag in tags}) for filename, file_tags in tag_data.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Then Go Make Films: A Conversation with Cale Glendening | Film + Music',\n",
       " \"Twitter user jokes how 'a bird cr*pped on a Smart Car and totalled it'... so firm proves him wrong with maths (and wins legions of fans for their efforts) | Daily Mail Online\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'JOBBLY CO.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = get_soup('1542621_raw_html.txt') # '1767762_raw_html.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'By                     Eddie Wrenn for MailOnline Published                      11 23 EST  21 June 2012                                          Updated                      01 53 EST  22 June 2012 45 View                        comments Social media has bought companies and their customers even closer together   which is both a good and bad thing  But when Twitter user Clayton Hove slung a wry comment onto the internet about Smart Car  he probably didn t expect such a dry  witty and obviously dedicated response  Hove simply told his 4 000 or so followers   Saw a bird had crapped on a Smart Car  Totaled it  It was intended as a throwaway remark   but Smart Car went to work to prove him wrong  calculating exactly how many pigeons it would take to  total  a Smart Car  What they came up with  it turns out  is 4 500 000   and a piece of social marketing genius which money cannot buy  Brilliant response  Smart Car  went on the attack  with one of the wittiest social media replies yet How many pigeons to total a Smart Car? 4 5million  Alternatively  that s 45 000  emu craps  The dedication given to a simple Tweet is pretty immense   the social marketing team put together a comparison chart of different animal  craps  added a few asterisks and footnotes  and even provided us with a key so we know exactly how many poops we are dealing with   But it was a point well made  and now   thanks to the image spreading on Facebook and Twitter   millions of people are now aware of the Smart Car s  tridion safety cell  which protects the driver as they motor around the place  Hove was both taken aback and proud of the response  admitting defeat at the hands of team  He replied   Outsmarted by Smart Car  Best  Social media response  Ever  Clayton replied a few hours later  with admiration for SmartCar Well played  social media team  As they gave us a chuckle  it s probably fair to show a picture of one of their cars  so here is the latest two seater  the Smart ForTwo   Share what you think The comments below have not been moderated  The views expressed in the contents above are those of our users and do not necessarily reflect the views of MailOnline  We are no longer accepting comments on this article  El Chapos ride to freedom  First pictures of motorbike on rails Mexican drugs lord used in tunnel escape from prison that he paid for with $50m in bribes Copy link to paste in your message Published by Associated Newspapers Ltd Part of the Daily Mail  The Mail on Sunday & Metro Media Group'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = re.sub(r'[\\'\"|\\n\\t,.:;()\\-\\/]+', ' ', ' '.join([p.text.strip() for p in soup.findAll('p')]))\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 'asas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asas'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import re\r\n",
      "import graphlab as gl\r\n",
      "from graphlab.toolkits.feature_engineering import TFIDF\r\n",
      "\r\n",
      "# put in the path to the kaggle data\r\n",
      "PATH_TO_JSON = \"path/to/data/from/process_html.py\"\r\n",
      "PATH_TO_TRAIN_LABELS = \"path/to/data/train.csv\"\r\n",
      "PATH_TO_TEST_LABELS = \"path/to/data/sampleSubmission.csv\"\r\n",
      "\r\n",
      "# a simple method to create some basic features on an SFrame\r\n",
      "def create_count_features(sf):\r\n",
      "    sf['num_images'] = sf['images'].apply(lambda x: len(x))\r\n",
      "    sf['num_links'] = sf['links'].apply(lambda x: len(x))\r\n",
      "    sf['num_clean_chars'] = sf['text_clean'].apply(lambda x: len(x))\r\n",
      "    return sf\r\n",
      "\r\n",
      "# a simple method to clean the text within an html response\r\n",
      "def clean_text(sf):\r\n",
      "    sf['text_clean'] = sf['text'].apply(lambda x:\r\n",
      "        re.sub(r'[\\n\\t,.:;()\\-\\/]+', ' ', ' '.join(x)))\r\n",
      "    sf['text_clean'] = sf['text_clean'].apply(lambda x: re.sub(r'\\s{2,}', ' ', x))\r\n",
      "    sf['text_clean'] = sf['text_clean'].apply(lambda x: x.strip())\r\n",
      "    return sf\r\n",
      "\r\n",
      "# a wrapper method around the 2 methods above\r\n",
      "def process_dataframe(sf):\r\n",
      "    sf = clean_text(sf)\r\n",
      "    sf = create_count_features(sf)\r\n",
      "    return sf \r\n",
      "\r\n",
      "# read json blocks from path PATH_TO_JSON\r\n",
      "sf = gl.SFrame.read_csv(PATH_TO_JSON, header=False)\r\n",
      "sf = sf.unpack('X1',column_name_prefix='')\r\n",
      "\r\n",
      "# read train and test labels from paths PATH_TO_TRAIN_LABELS and PATH_TO_TEST_LABELS\r\n",
      "train_labels = gl.SFrame.read_csv(PATH_TO_TRAIN_LABELS)\r\n",
      "test_labels = gl.SFrame.read_csv(PATH_TO_TEST_LABELS)\r\n",
      "\r\n",
      "# create a new columns \"id\" from parsing urlId and drop file columns\r\n",
      "train_labels['id'] = train_labels['file'].apply(lambda x: str(x.split('_')[0] ))\r\n",
      "train_labels = train_labels.remove_column('file')\r\n",
      "test_labels['id'] = test_labels['file'].apply(lambda x: str(x.split('_')[0] ))\r\n",
      "test_labels = test_labels.remove_column('file')\r\n",
      "\r\n",
      "# join labels with html data from training and testing SFrames\r\n",
      "train = train_labels.join(sf, on='id', how='left')\r\n",
      "test = test_labels.join(sf, on='id', how='left')\r\n",
      "\r\n",
      "# call wrapper method process_dataframe on train/test\r\n",
      "train =  process_dataframe(train)\r\n",
      "test = process_dataframe(test)\r\n",
      "\r\n",
      "# create word counts and remove countwords for both train and test\r\n",
      "bow_trn = gl.text_analytics.count_words(train['text_clean'])\r\n",
      "bow_trn = bow_trn.dict_trim_by_keys(gl.text_analytics.stopwords())\r\n",
      "\r\n",
      "bow_tst = gl.text_analytics.count_words(test['text_clean'])\r\n",
      "bow_tst = bow_tst.dict_trim_by_keys(gl.text_analytics.stopwords())\r\n",
      "\r\n",
      "# add the bag of words to both sframes\r\n",
      "train['bow'] = bow_trn\r\n",
      "test['bow'] = bow_tst\r\n",
      "\r\n",
      "# create a TFIDF Transformer that is fit on your training data and transform both training and testing data\r\n",
      "encoder = gl.feature_engineering.create(train, TFIDF('bow', output_column_name='tfidf'))\r\n",
      "train = encoder.transform(train)\r\n",
      "test = encoder.transform(test)\r\n",
      "\r\n",
      "# train logistic regression on training data with tf-idf as features and predict on testing data\r\n",
      "train = train.dropna()\r\n",
      "model = gl.logistic_classifier.create(train, target='sponsored', features=['tfidf'], class_weights='auto')\r\n",
      "\r\n",
      "test = test.dropna()\r\n",
      "ypred = model.predict(test)\r\n",
      "\r\n",
      "# create submission.csv\r\n",
      "submission = gl.SFrame()\r\n",
      "submission['sponsored'] = ypred \r\n",
      "submission['file'] = test['id'].apply(lambda x: x + '_raw_html.txt')\r\n",
      "submission.save('submission_version_1.csv', format='csv')"
     ]
    }
   ],
   "source": [
    "!cat /Users/rbekbolatov/repos/gh/bekbolatov/kaggle/events/native/classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
