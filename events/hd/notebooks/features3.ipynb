{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn import pipeline, model_selection\n",
    "from sklearn import pipeline, grid_search\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "import re\n",
    "import random\n",
    "random.seed(2016)\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOC = '/home/ec2-user/data/hd/unpacked/'\n",
    "df_train = pd.read_csv(LOC + 'train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv(LOC + 'test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv(LOC + 'product_descriptions.csv', encoding=\"ISO-8859-1\")\n",
    "df_attr = pd.read_csv(LOC + 'attributes.csv', encoding=\"ISO-8859-1\")\n",
    "df_matches = pd.read_csv(LOC + 'matched_strings_clean.csv').fillna(\"\")\n",
    "\n",
    "df_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"}).fillna(\"\")\n",
    "num_train = df_train.shape[0]\n",
    "# (74067, 5), (166693, 4) -> df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True) # (240760, 5)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_matches, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attr_len = df_attr.groupby('product_uid', as_index=False)['name'].agg({'attr_count':(lambda x: len(list(x)))})\n",
    "df_all = pd.merge(df_all, df_attr_len, how='left', on='product_uid')\n",
    "df_all['attr_count'] = df_all['attr_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                 u'id',       u'product_title',         u'product_uid',\n",
       "                 u'relevance',         u'search_term', u'product_description',\n",
       "                     u'brand',                 u'tit',                u'tit2',\n",
       "                      u'desc',               u'desc2',          u'attributes',\n",
       "                  u'mfgbrand',           u'mfgbrand2',          u'attr_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_all.drop('list', axis=1, inplace=True)\n",
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern_camel = re.compile(r\"([a-z]+)([0-9A]|([A-Z][^ ]+))\")\n",
    "pattern_lcase_number = re.compile(r\"([a-z])([0-9])\")\n",
    "pattern_digit_lcase = re.compile(r\"([0-9])([a-z])\")\n",
    "pattern_s = re.compile(r\"([a-z])'s\")\n",
    "pattern_number_commas = re.compile(r\"([0-9]),([0-9])\")\n",
    "\n",
    "    \n",
    "# 4x2\n",
    "XBY = \"xby\"\n",
    "pattern_xby_d = re.compile(r\"(x[0-9])\")\n",
    "pattern_d_xby = re.compile(r\"([0-9])x\")\n",
    "\n",
    "# units\n",
    "pattern_inch = re.compile(r\"([0-9])( *)(inches|inch|in|')\\.?\")\n",
    "pattern_foot = re.compile(r\"([0-9])( *)(foot|feet|ft|''|\\\")\\.?\")\n",
    "pattern_pound = re.compile(r\"([0-9])( *)(pounds|pound|lbs|lb)\\.?\")\n",
    "pattern_sqft = re.compile(r\"([0-9])( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\")\n",
    "pattern_gallons = re.compile(r\"([0-9])( *)(gallons|gallon|gal)\\.?\")\n",
    "pattern_oz = re.compile(r\"([0-9])( *)(ounces|ounce|oz)\\.?\")\n",
    "pattern_cm = re.compile(r\"([0-9])( *)(centimeters|cm)\\.?\")\n",
    "pattern_mm = re.compile(r\"([0-9])( *)(milimeters|mm)\\.?\")\n",
    "pattern_deg = re.compile(r\"([0-9])( *)(degrees|degree)\\.?\")\n",
    "pattern_volt = re.compile(r\"([0-9])( *)(volts|volt)\\.?\")\n",
    "pattern_watt = re.compile(r\"([0-9])( *)(watts|watt)\\.?\")\n",
    "pattern_amp = re.compile(r\"([0-9])( *)(amperes|ampere|amps|amp)\\.?\")\n",
    "pattern_kamp = re.compile(r\"([0-9])( *)(kiloamperes|kiloampere|kamps|kamp|ka)\\.?\")\n",
    "\n",
    "# split\n",
    "pattern_split = re.compile('[^0-9a-z]')\n",
    "\n",
    "known_words = set([\"the\", \"a\", \"an\",\n",
    "    \"this\", \"that\", \"which\", \"whose\",\n",
    "    \"other\", \"and\", \"or\",\n",
    "    \"be\", \"is\", \"are\", \"been\",\n",
    "    \"have\", \"has\", \"had\",\n",
    "    \"can\", \"could\", \"will\", \"would\",\n",
    "    \"go\", \"gone\", \"see\", \"seen\",\n",
    "    \"all\", \"some\", \"any\", \"most\", \"several\", \"no\", \"none\", \"nothing\",\n",
    "    \"as\", \"of\", \"in\", \"on\", \"at\", \"over\", \"from\", \"to\",\n",
    "    \"with\", \"through\", \"for\", \"when\", \"then\",\n",
    "    \"new\", \"old\",\n",
    "    \"you\", \"your\", \"yours\", \"me\", \"i\", \"my\", \"mine\", \"it\", \"its\"])\n",
    "\n",
    "def str_stem(s): \n",
    "    if isinstance(s, str) or isinstance(s, unicode):\n",
    "        \n",
    "        s = pattern_camel.sub(r\"\\1 \\2\", s)\n",
    "        s = pattern_lcase_number.sub(r\"\\1 \\2\", s)\n",
    "        s = pattern_digit_lcase.sub(r\"\\1 \\2\", s)\n",
    "        s = pattern_number_commas.sub(r\"\\1\\2\", s)\n",
    "        s = pattern_s.sub(r\"\\1\", s)\n",
    "        \n",
    "        \n",
    "        s = s.lower().strip()\n",
    "        \n",
    "        # 4ft x 2ft\n",
    "        s = s.replace(\" x \",\" \" + XBY + \" \")\n",
    "        s = s.replace(\"*\",\" \" + XBY + \" \")        \n",
    "        s = s.replace(\" by \",\" \" + XBY)\n",
    "        s = pattern_xby_d.sub(\" \" + XBY + \" \\1\", s)\n",
    "        s = pattern_d_xby.sub(\"\\1 \" + XBY + \" \", s)\n",
    "        \n",
    "        # units\n",
    "        s = pattern_inch.sub(r\"\\1 inch \", s)\n",
    "        s = pattern_foot.sub(r\"\\1 foot \", s)\n",
    "        s = pattern_pound.sub(r\"\\1 pound \", s)\n",
    "        s = pattern_sqft.sub(r\"\\1 sqft \", s)\n",
    "        s = pattern_gallons.sub(r\"\\1 gal \", s)\n",
    "        s = pattern_oz.sub(r\"\\1 oz \", s)\n",
    "        s = pattern_cm.sub(r\"\\1 cm \", s)\n",
    "        s = pattern_mm.sub(r\"\\1 mm \", s)\n",
    "        s = pattern_deg.sub(r\"\\1 deg \", s)\n",
    "        s = pattern_volt.sub(r\"\\1 volt \", s)\n",
    "        s = pattern_watt.sub(r\"\\1 watt \", s)\n",
    "        s = pattern_amp.sub(r\"\\1 amp \", s)\n",
    "        s = pattern_kamp.sub(r\"\\1 kamp \", s)\n",
    "        \n",
    "        # some by hand\n",
    "        s = s.replace(\"whirpool\",\"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolga\", \"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolstainless\",\"whirlpool stainless\")\n",
    "        s = s.replace(\"pressure-treated\",\"pressure treated pt\")\n",
    "        \n",
    "        s = ' '.join([x for x in pattern_split.split(s) if x and x not in known_words])\n",
    "        return s\n",
    "    else:\n",
    "        #raise ValueError(\"Type of \" + str(s) + \" is \" + str(type(s)))\n",
    "        #print \"HUY\"\n",
    "        return 'null'\n",
    "    \n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:str_stem(x))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:str_stem(x))\n",
    "df_all['product_description'] = df_all['product_description'].map(lambda x:str_stem(x))\n",
    "df_all['brand'] = df_all['brand'].map(lambda x:str_stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_common_word(str1, str2):\n",
    "    words, cnt = str1.split(), 0\n",
    "    for word in words:\n",
    "        if str2.find(word)>=0:\n",
    "            cnt+=1\n",
    "    return cnt\n",
    "\n",
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# id\tproduct_title\tproduct_uid\trelevance\tsearch_term\tproduct_description\tbrand\n",
    "# id, relevance, search_term, product_title, product_description, (product_uid,) brand  [product_info, attr]\n",
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, hd_searches):\n",
    "        d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand'] + \\\n",
    "        ['tit', 'tit2', 'desc', 'desc2', 'attributes', 'mfgbrand', 'mfgbrand2'] + \\\n",
    "        ['brand_feature'] #['ratio_brand']\n",
    "        #[] #['ratio_title', 'ratio_description', 'ratio_brand']\n",
    "        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n",
    "        return hd_searches\n",
    "\n",
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].apply(str)\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "def fmse(ground_truth, predictions):\n",
    "    return mean_squared_error(ground_truth, predictions)\n",
    "\n",
    "#RMSE  = make_scorer(fmse, greater_is_better=False)\n",
    "RMSE  = make_scorer(fmean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_word(r):\n",
    "    title = r['product_title'].split()\n",
    "    ms = r['tit'].split(\",\")\n",
    "    ic = -1\n",
    "    for m in ms:\n",
    "        for i, t in enumerate(title):\n",
    "            if m == t and i > ic:\n",
    "                ic = i\n",
    "    if ic < 0:\n",
    "        ic = 10\n",
    "    else:\n",
    "        ic = len(title) - ic - 1\n",
    "    return ic #, ms, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_word_match(r):\n",
    "    title = r['product_title'].split()\n",
    "    ms = r['tit'].split(\",\")\n",
    "    if len(title) > 0 and len(ms) > 0 and title[-1] == ms[-1]:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "   return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word, NWORDS):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words, NWORDS): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word, NWORDS):\n",
    "    candidates = known([word], NWORDS) or known(edits1(word), NWORDS) or known_edits2(word, NWORDS) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "already_matched = {}\n",
    "def similar_words(w1, w2):\n",
    "    if (w1, w2) in already_matched:\n",
    "        return already_matched[(w1, w2)]\n",
    "    if abs(len(w1) - len(w2)) > 2:\n",
    "        res = False\n",
    "    elif len(set(w1).difference(set(w2))) > 2 or len(set(w2).difference(set(w1))) > 2 :\n",
    "        res = False\n",
    "    else:\n",
    "        res = w2 == correct(w1, {w2:1})\n",
    "    already_matched[(w1, w2)] = res\n",
    "    return res\n",
    "    \n",
    "def last_word_match_query_last(r):\n",
    "    title = r['product_title'].split(' ')\n",
    "    ms = r['search_term'].split(' ')\n",
    "    if len(title) > 0 and len(ms) > 0 and similar_words(title[-1], ms[-1]):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features Set: 3.31 minutes ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#comment out the lines below use df_all.csv for further grid search testing\n",
    "#if adding features consider any drops on the 'cust_regression_vals' class\n",
    "\n",
    "df_all['product_info'] = df_all['search_term']+\"\\t\"+df_all['product_title'] +\"\\t\"+df_all['product_description']\n",
    "df_all['attr'] = df_all['search_term']+\"\\t\"+df_all['brand']\n",
    "\n",
    "df_all['len_of_query'] = df_all['search_term'].map(lambda x: max(1, len(x.split()))).astype(np.int64)\n",
    "df_all['len_of_title'] = df_all['product_title'].map(lambda x: len(x.split())).astype(np.int64)\n",
    "df_all['len_of_description'] = df_all['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_brand'] = df_all['brand'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "\n",
    "df_all['letters_query'] = df_all['search_term'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_title'] = df_all['product_title'].map(lambda x:len(x)).astype(np.int64)\n",
    "df_all['letters_desc'] = df_all['product_description'].map(lambda x:len(x)).astype(np.int64)\n",
    "df_all['letters_brand'] = df_all['brand'].map(lambda x:len(x)).astype(np.int64)\n",
    "\n",
    "###############################\n",
    "# df_all['query_in_title'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "# df_all['query_in_description'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))\n",
    "\n",
    "# df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "# df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "# df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "\n",
    "df_all['query_in_title'] = df_all['tit'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_description'] = df_all['desc'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_attrs'] = df_all['attributes'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_brand'] = df_all['mfgbrand'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "\n",
    "df_all['letters_query_in_title'] = df_all['tit'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_query_in_description'] = df_all['desc'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_query_in_attrs'] = df_all['attributes'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_query_in_brand'] = df_all['mfgbrand'].map(lambda x: len(x)).astype(np.int64)\n",
    "\n",
    "\n",
    "df_all['query_in_title2'] = df_all['tit2'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_description2'] = df_all['desc2'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_brand2'] = df_all['mfgbrand2'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "\n",
    "\n",
    "df_all['letters_query_in_title2'] = df_all['tit2'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_query_in_description2'] = df_all['desc2'].map(lambda x: len(x)).astype(np.int64)\n",
    "\n",
    "df_all['ratio_letters_query_in_title'] = df_all['letters_query_in_title2']/(df_all['letters_query_in_title'] + 1)\n",
    "df_all['ratio_letters_query_in_descr'] = df_all['letters_query_in_description2']/(df_all['letters_query_in_description'] + 1)\n",
    "\n",
    "\n",
    "df_all['query_in_title_num'] = df_all['tit'].map(lambda y: len([x for x in y.split(\",\") if x and re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_description_num'] = df_all['desc'].map(lambda y: len([x for x in y.split(\",\") if x and re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_attrs_num'] = df_all['attributes'].map(lambda y: len([x for x in y.split(\",\") if x and re.match(r'^[0-9]+$', x)]))\n",
    "\n",
    "\n",
    "\n",
    "#df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "#df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "#df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "###############################\n",
    "\n",
    "\n",
    "df_all['ratio_title'] = df_all['query_in_title']/df_all['len_of_query']\n",
    "df_all['ratio_description'] = df_all['query_in_description']/df_all['len_of_query']\n",
    "\n",
    "# df_all['ratio_title'] = df_all['word_in_title']/df_all['len_of_query']\n",
    "# df_all['ratio_description'] = df_all['word_in_description']/df_all['len_of_query']\n",
    "# df_all['ratio_brand'] = df_all['word_in_brand']/df_all['len_of_brand']\n",
    "\n",
    "\n",
    "\n",
    "df_all['title_match_last_title_word'] = df_all.apply(last_word, axis=1)\n",
    "df_all['title_match_last_word'] = df_all.apply(last_word_match, axis=1)\n",
    "df_all['title_match_last_word_query'] = df_all.apply(last_word_match_query_last, axis=1)\n",
    "\n",
    "df_brand = pd.unique(df_all.brand.ravel())\n",
    "d={}\n",
    "i = 1\n",
    "for s in df_brand:\n",
    "    d[s]=i\n",
    "    i+=1\n",
    "df_all['brand_feature'] = df_all['brand'].map(lambda x: d[x])\n",
    "#df_all['search_term_feature'] = df_all['search_term'].map(lambda x:len(x))\n",
    "\n",
    "#df_all.to_csv('df_all_322_1.csv')\n",
    "#df_all = pd.read_csv('df_all.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "\n",
    "df_train = df_all.iloc[:num_train]\n",
    "df_test = df_all.iloc[num_train:]\n",
    "id_test = df_test['id']\n",
    "y_train = df_train['relevance'].values\n",
    "X_train = df_train[:]\n",
    "X_test = df_test[:]\n",
    "print(\"--- Features Set: %s minutes ---\" % round(((time.time() - start_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_all[1000:1030]\n",
    "#df_all['query_in_title'] + 1\n",
    "#df_all['query_in_title2']/(df_all['query_in_title'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD FROM SAVED\n",
    "#df_all = pd.read_csv('df_all.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "#df_all = pd.read_csv('df_all_322_1.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "\n",
    "df_train = df_all.iloc[:num_train]\n",
    "df_test = df_all.iloc[num_train:]\n",
    "id_test = df_test['id']\n",
    "y_train = df_train['relevance'].values\n",
    "X_train = df_train[:]\n",
    "X_test = df_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_all[300:320][['relevance', 'product_title', 'search_term', 'product_description']]\n",
    "#X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "tsvd = TruncatedSVD(n_components=10, random_state = 2016)\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# dictvect = DictVectorizer()\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohenc = OneHotEncoder()\n",
    "randomForestRegressor = RandomForestRegressor(n_estimators = 500, min_samples_leaf=3, n_jobs = -1, random_state = 5017, verbose = 1)\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "                    transformer_list = [\n",
    "                        ('cst',  cust_regression_vals()),  \n",
    "                    \n",
    "                        ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n",
    "                        ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),\n",
    "                        ('txt3', pipeline.Pipeline([('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf), ('tsvd3', tsvd)])),\n",
    "                        ('txt4', pipeline.Pipeline([('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf), ('tsvd4', tsvd)]))\n",
    "                    \n",
    "#                         ('txt1', pipeline.Pipeline([ ('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf)  ])),\n",
    "#                         ('txt2', pipeline.Pipeline([ ('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf)  ])),\n",
    "#                         ('txt3', pipeline.Pipeline([ ('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf) ])),\n",
    "#                         ('txt4', pipeline.Pipeline([ ('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf) ]))\n",
    "                    \n",
    "#                         ('brandf', pipeline.Pipeline([ ('s5', cust_txt_col(key='brand_feature')), ('ohenc', ohenc)  ])),\n",
    "                        ],\n",
    "                    transformer_weights = {\n",
    "                        'cst': 1.0,\n",
    "                        'txt1': 0.5,\n",
    "                        'txt2': 0.25,\n",
    "                        'txt3': 0.5,\n",
    "                        'txt4': 0.5\n",
    "#                         'brandf': 1.0\n",
    "                        },\n",
    "                n_jobs = -1\n",
    "                ))\n",
    "#         , \n",
    "#         ('rfr', randomForestRegressor)\n",
    "    ])\n",
    "\n",
    "#clf.set_params(rfr__max_features=10, rfr__max_depth=20)\n",
    "#clf.fit(X_train, y_train)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00001000e+05,   1.50000000e+01,   2.00000000e+00, ...,\n",
       "          6.49899457e-05,  -1.09856277e-03,  -6.84634068e-03],\n",
       "       [  1.00001000e+05,   1.50000000e+01,   2.00000000e+00, ...,\n",
       "         -3.87713348e-06,  -1.13283073e-03,  -6.72812595e-03],\n",
       "       [  1.00002000e+05,   3.50000000e+01,   1.00000000e+00, ...,\n",
       "         -5.63536932e-03,   2.92041213e-03,  -5.97269368e-03],\n",
       "       ..., \n",
       "       [  2.06641000e+05,   4.80000000e+01,   6.00000000e+00, ...,\n",
       "         -1.36185457e-06,   2.63435202e-05,   8.18920971e-06],\n",
       "       [  2.06648000e+05,   1.40000000e+01,   3.00000000e+00, ...,\n",
       "         -6.40421081e-16,  -3.94993107e-16,  -1.12821783e-15],\n",
       "       [  2.06650000e+05,   3.30000000e+01,   5.00000000e+00, ...,\n",
       "         -7.15176477e-10,  -7.51147525e-08,   5.51772459e-08]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = clf.fit_transform(X_train)\n",
    "#model.fit(X_train, y_train)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = clf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix( a, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain.save_binary(\"train.buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(b)\n",
    "dtest.save_binary(\"test.buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evallist  = [(dtrain,'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param = {'max_depth':2, \n",
    "         'eta':3, \n",
    "#          'objective':'reg:linear',\n",
    "         'eval_metric':'rmse',\n",
    "         'maximize': False,\n",
    "         'colsample_bytree':1,\n",
    "         'subsample':1,\n",
    "         'nthread':4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_round = 4\n",
    "bst = xgb.train( param, dtrain, num_round, evallist, early_stopping_rounds=15, verbose_eval=2)\n",
    "bst.best_iteration, bst.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test-error-mean  test-error-std  train-error-mean  train-error-std\n",
       "0        -1.381635        0.002793         -1.381635         0.002793\n",
       "1        -1.381635        0.002793         -1.381635         0.002793\n",
       "2        -1.381635        0.002793         -1.381635         0.002793\n",
       "3        -1.381635        0.002793         -1.381635         0.002793"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plst = list(param.items())\n",
    "xgb.cv(param, dtrain, num_boost_round=num_round, nfold=2, metrics={'error'}, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reg_alpha',\n",
       " 'colsample_bytree',\n",
       " 'silent',\n",
       " 'colsample_bylevel',\n",
       " 'scale_pos_weight',\n",
       " 'learning_rate',\n",
       " 'missing',\n",
       " 'max_delta_step',\n",
       " 'nthread',\n",
       " 'base_score',\n",
       " 'n_estimators',\n",
       " 'subsample',\n",
       " 'reg_lambda',\n",
       " 'seed',\n",
       " 'min_child_weight',\n",
       " 'objective',\n",
       " 'max_depth',\n",
       " 'gamma']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(param)\n",
    "paramso = {'max_depth': [2], \n",
    "         'learning_rate': [3], \n",
    "         'colsample_bytree':[1],\n",
    "         'subsample':[1]\n",
    "        }\n",
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "model = grid_search.GridSearchCV(estimator = clf, param_grid = paramso, n_jobs = -1, cv = 5, verbose = 20, scoring=RMSE)\n",
    "model.fit(a, y_train)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.27597213,  2.03546667,  2.45794249, ...,  1.62093401,\n",
       "        2.60387492,  2.5656836 ], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  1.5min remaining:  -15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  1.5min remaining:  -15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  1.5min remaining:  -15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  1.9min remaining:  -19.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   20.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   27.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search:\n",
      "{'max_features': 2, 'max_depth': 30}\n",
      "Best CV score:\n",
      "-0.466961553969\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] .......... max_features=2, max_depth=30, score=-0.474227 - 1.5min[CV] .......... max_features=2, max_depth=30, score=-0.461016 - 1.5min[CV] .......... max_features=2, max_depth=30, score=-0.455927 - 1.5min[CV] .......... max_features=2, max_depth=30, score=-0.464262 - 1.5min\n",
      "\n",
      "\n",
      "\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] .......... max_features=2, max_depth=30, score=-0.479375 -  22.6s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_features': [2], 'max_depth': [30]}\n",
    "model = grid_search.GridSearchCV(estimator = randomForestRegressor, param_grid = param_grid, n_jobs = -1, cv = 5, verbose = 20, scoring=RMSE)\n",
    "model.fit(a, y_train)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  2.3min remaining:  -23.3s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  2.3min remaining:  -23.5s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  2.4min remaining:  -23.8s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  3.1min remaining:  -30.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.1min finished\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.5min finished\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   24.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.481643 - 2.2min[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.466586 - 2.3min[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.461499 - 2.2min[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.469350 - 2.2min\n",
      "\n",
      "\n",
      "\n",
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.486730 -  45.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   31.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training: 4.04 minutes ---\n",
      "Best parameters found by grid search:\n",
      "{'rfr__max_features': 2, 'rfr__max_depth': 30}\n",
      "Best CV score:\n",
      "-0.473161739747\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "param_grid = {'rfr__max_features': [2], 'rfr__max_depth': [30]}\n",
    "model = grid_search.GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 5, verbose = 20, scoring=RMSE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Training: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train.ix[3782]\n",
    "inds = pd.isnull(X_train).any(1).nonzero()[0]\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.isfinite(X_train.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.isfinite(X_train).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search:\n",
      "{'rfr__max_features': 2, 'rfr__max_depth': 30}\n",
      "Best CV score:\n",
      "-0.468286123481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    6.0s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission_x.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmi = pd.read_csv('./submission6_50t_50m.csv', encoding=\"ISO-8859-1\")\n",
    "fo = pd.read_csv('./submission_x.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.006440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.045906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.149894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.496521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.222372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance\n",
       "0   1   2.006440\n",
       "1   4   2.045906\n",
       "2   5   2.149894\n",
       "3   6   2.496521\n",
       "4   7   2.222372"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.055402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.163738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.175020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.651792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.345252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance\n",
       "0   1   2.055402\n",
       "1   4   2.163738\n",
       "2   5   2.175020\n",
       "3   6   2.651792\n",
       "4   7   2.345252"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = pd.merge(fo, fmi, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance_x</th>\n",
       "      <th>relevance_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.055402</td>\n",
       "      <td>2.006440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.163738</td>\n",
       "      <td>2.045906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.175020</td>\n",
       "      <td>2.149894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.651792</td>\n",
       "      <td>2.496521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.345252</td>\n",
       "      <td>2.222372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance_x  relevance_y\n",
       "0   1     2.055402     2.006440\n",
       "1   4     2.163738     2.045906\n",
       "2   5     2.175020     2.149894\n",
       "3   6     2.651792     2.496521\n",
       "4   7     2.345252     2.222372"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(r):\n",
    "    return r['relevance_x']*0.1 + r['relevance_y']*0.9\n",
    "\n",
    "y_pred = res.apply(f, axis=1)\n",
    "pd.DataFrame({\"id\": res['id'], \"relevance\": y_pred}).to_csv('submission_x_10_90.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
