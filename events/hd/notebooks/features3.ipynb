{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn import pipeline, model_selection\n",
    "from sklearn import pipeline, grid_search\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "import re\n",
    "import random\n",
    "random.seed(2016)\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LOC = '/home/ec2-user/data/hd/unpacked/'\n",
    "LOC = '/Users/rbekbolatov/data/kaggle/homedepot/'\n",
    "df_train = pd.read_csv(LOC + 'train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv(LOC + 'test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv(LOC + 'product_descriptions.csv', encoding=\"ISO-8859-1\")\n",
    "df_attr = pd.read_csv(LOC + 'attributes.csv', encoding=\"ISO-8859-1\")\n",
    "df_matches = pd.read_csv(LOC + 'matched_strings_clean.csv').fillna(\"\")\n",
    "\n",
    "df_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"}).fillna(\"EMPTY\")\n",
    "num_train = df_train.shape[0]\n",
    "# (74067, 5), (166693, 4) -> df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True) # (240760, 5)\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194908"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['brand'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198       1.0\n",
       "199       1.0\n",
       "219       1.0\n",
       "302       1.0\n",
       "881       1.0\n",
       "1208      1.0\n",
       "1219      1.0\n",
       "1241      1.0\n",
       "1242      1.0\n",
       "1419      1.0\n",
       "1861      1.0\n",
       "1862      1.0\n",
       "1863      1.0\n",
       "1864      1.0\n",
       "1865      1.0\n",
       "2007      1.0\n",
       "2275      1.0\n",
       "2276      1.0\n",
       "2277      1.0\n",
       "2833      1.0\n",
       "3443      1.0\n",
       "3563      1.0\n",
       "3564      1.0\n",
       "3758      1.0\n",
       "4348      1.0\n",
       "4578      1.0\n",
       "4579      1.0\n",
       "4840      1.0\n",
       "5116      1.0\n",
       "5513      1.0\n",
       "         ... \n",
       "240167    1.0\n",
       "240172    1.0\n",
       "240194    1.0\n",
       "240199    1.0\n",
       "240206    1.0\n",
       "240209    1.0\n",
       "240258    1.0\n",
       "240260    1.0\n",
       "240276    1.0\n",
       "240318    1.0\n",
       "240341    1.0\n",
       "240427    1.0\n",
       "240428    1.0\n",
       "240489    1.0\n",
       "240494    1.0\n",
       "240508    1.0\n",
       "240511    1.0\n",
       "240530    1.0\n",
       "240545    1.0\n",
       "240548    1.0\n",
       "240560    1.0\n",
       "240584    1.0\n",
       "240633    1.0\n",
       "240658    1.0\n",
       "240670    1.0\n",
       "240673    1.0\n",
       "240696    1.0\n",
       "240749    1.0\n",
       "240757    1.0\n",
       "240758    1.0\n",
       "Name: brand, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[df_all['brand'].isnull()]['brand'].apply(lambda s: 1.0 if pd.isnull(s) else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.0\n",
       "2  0.0  0.0  0.0  0.0  0.0\n",
       "3  0.0  0.0  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def brand_features(s):\n",
    "    a = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    if pd.isnull(s):\n",
    "        a[0] = 1.0\n",
    "    elif s == 'Unbranded':\n",
    "        a[1] = 1.0\n",
    "    elif s == 'Hampton Bay':\n",
    "        a[2] = 1.0\n",
    "    elif s == 'GE':\n",
    "        a[3] = 1.0\n",
    "    elif s == 'Everbilt':\n",
    "        a[4] = 1.0\n",
    "    return a\n",
    "\n",
    "df_brand_features = pd.DataFrame(df_all['brand'].apply(lambda s: brand_features(s)).tolist())\n",
    "\n",
    "df_train = df_brand_features.iloc[:num_train]\n",
    "df_test = df_brand_features.iloc[num_train:]\n",
    "\n",
    "df_train\n",
    "\n",
    "df_train.head()\n",
    "#df_all['brand'].apply(lambda s: brand_features(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = np.array(df_train)\n",
    "df_test = np.array(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('features_brand_01_train', df_train)\n",
    "np.save('features_brand_01_test', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('features_brand_01_train.csv')\n",
    "df_test.to_csv('features_brand_01_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "Name: brand, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unbranded                     7766\n",
       "Hampton Bay                   5173\n",
       "GE                            3012\n",
       "Everbilt                      2819\n",
       "Home Decorators Collection    2608\n",
       "KOHLER                        2580\n",
       "DEWALT                        2034\n",
       "Delta                         2029\n",
       "Ryobi                         1903\n",
       "Prime-Line                    1754\n",
       "Philips                       1634\n",
       "Glacier Bay                   1631\n",
       "Milwaukee                     1550\n",
       "Veranda                       1387\n",
       "RIDGID                        1359\n",
       "Whirlpool                     1316\n",
       "Crown Bolt                    1294\n",
       "Husky                         1247\n",
       "MOEN                          1243\n",
       "American Standard             1240\n",
       "Daltile                       1212\n",
       "Lithonia Lighting             1178\n",
       "LG Electronics                1110\n",
       "HDX                           1077\n",
       "MS International               971\n",
       "Leviton                        913\n",
       "Makita                         877\n",
       "BLACK+DECKER                   872\n",
       "Masonite                       868\n",
       "Simpson Strong-Tie             855\n",
       "                              ... \n",
       "POLLENTEC                        1\n",
       "Cut-N-Crown                      1\n",
       "Durham's Rock Hard               1\n",
       "Kryton Hydropel                  1\n",
       "KingsBottle                      1\n",
       "Makinex                          1\n",
       "MIO                              1\n",
       "LITTLE RAPIDS                    1\n",
       "IRONFORCE                        1\n",
       "Enigma                           1\n",
       "Absolute Zero                    1\n",
       "Viva                             1\n",
       "Polaris                          1\n",
       "Aleene's                         1\n",
       "Zibra                            1\n",
       "SWISSGEAR                        1\n",
       "Aurora Lighting                  1\n",
       "ThermoSoft WarmFlex              1\n",
       "Perfect Garden Tool              1\n",
       "Patriot                          1\n",
       "Bayco                            1\n",
       "Flexgrain                        1\n",
       "Byers'                           1\n",
       "Nesco                            1\n",
       "AquaLife                         1\n",
       "Acme Sponge & Chamois            1\n",
       "Weed Eater                       1\n",
       "Gladon                           1\n",
       "Copper Mountain Hardware         1\n",
       "Kimtech                          1\n",
       "Name: brand, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True) # (240760, 5)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_matches, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attr_len = df_attr.groupby('product_uid', as_index=False)['name'].agg({'attr_count':(lambda x: len(list(x)))})\n",
    "df_all = pd.merge(df_all, df_attr_len, how='left', on='product_uid')\n",
    "df_all['attr_count'] = df_all['attr_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                 u'id',       u'product_title',         u'product_uid',\n",
       "                 u'relevance',         u'search_term', u'product_description',\n",
       "                     u'brand',                 u'tit',                u'tit2',\n",
       "                      u'desc',               u'desc2',          u'attributes',\n",
       "                  u'mfgbrand',           u'mfgbrand2',          u'attr_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_all.drop('list', axis=1, inplace=True)\n",
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern_camel = re.compile(r\"([a-z]+)([0-9A]|([A-Z][^ ]+))\")\n",
    "pattern_lcase_number = re.compile(r\"([a-z])([0-9])\")\n",
    "pattern_digit_lcase = re.compile(r\"([0-9])([a-z])\")\n",
    "pattern_s = re.compile(r\"([a-z])'s\")\n",
    "pattern_number_commas = re.compile(r\"([0-9]),([0-9])\")\n",
    "\n",
    "    \n",
    "# 4x2\n",
    "XBY = \"xby\"\n",
    "pattern_xby_d = re.compile(r\"(x[0-9])\")\n",
    "pattern_d_xby = re.compile(r\"([0-9])x\")\n",
    "\n",
    "# units\n",
    "pattern_inch = re.compile(r\"([0-9])( *)(inches|inch|in|')\\.?\")\n",
    "pattern_foot = re.compile(r\"([0-9])( *)(foot|feet|ft|''|\\\")\\.?\")\n",
    "pattern_pound = re.compile(r\"([0-9])( *)(pounds|pound|lbs|lb)\\.?\")\n",
    "pattern_sqft = re.compile(r\"([0-9])( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\")\n",
    "pattern_gallons = re.compile(r\"([0-9])( *)(gallons|gallon|gal)\\.?\")\n",
    "pattern_oz = re.compile(r\"([0-9])( *)(ounces|ounce|oz)\\.?\")\n",
    "pattern_cm = re.compile(r\"([0-9])( *)(centimeters|cm)\\.?\")\n",
    "pattern_mm = re.compile(r\"([0-9])( *)(milimeters|mm)\\.?\")\n",
    "pattern_deg = re.compile(r\"([0-9])( *)(degrees|degree)\\.?\")\n",
    "pattern_volt = re.compile(r\"([0-9])( *)(volts|volt)\\.?\")\n",
    "pattern_watt = re.compile(r\"([0-9])( *)(watts|watt)\\.?\")\n",
    "pattern_amp = re.compile(r\"([0-9])( *)(amperes|ampere|amps|amp)\\.?\")\n",
    "pattern_kamp = re.compile(r\"([0-9])( *)(kiloamperes|kiloampere|kamps|kamp|ka)\\.?\")\n",
    "\n",
    "# split\n",
    "pattern_split = re.compile('[^0-9a-z]')\n",
    "\n",
    "known_words = set([\"the\", \"a\", \"an\",\n",
    "    \"this\", \"that\", \"which\", \"whose\",\n",
    "    \"other\", \"and\", \"or\",\n",
    "    \"be\", \"is\", \"are\", \"been\",\n",
    "    \"have\", \"has\", \"had\",\n",
    "    \"can\", \"could\", \"will\", \"would\",\n",
    "    \"go\", \"gone\", \"see\", \"seen\",\n",
    "    \"all\", \"some\", \"any\", \"most\", \"several\", \"no\", \"none\", \"nothing\",\n",
    "    \"as\", \"of\", \"in\", \"on\", \"at\", \"over\", \"from\", \"to\",\n",
    "    \"with\", \"through\", \"for\", \"when\", \"then\",\n",
    "    \"new\", \"old\",\n",
    "    \"you\", \"your\", \"yours\", \"me\", \"i\", \"my\", \"mine\", \"it\", \"its\"])\n",
    "\n",
    "def str_stem(s): \n",
    "    if isinstance(s, str) or isinstance(s, unicode):\n",
    "        \n",
    "        s = pattern_camel.sub(r\"\\1 \\2\", s)\n",
    "        s = pattern_lcase_number.sub(r\"\\1 \\2\", s)\n",
    "        s = pattern_digit_lcase.sub(r\"\\1 \\2\", s)\n",
    "        s = pattern_number_commas.sub(r\"\\1\\2\", s)\n",
    "        s = pattern_s.sub(r\"\\1\", s)\n",
    "        \n",
    "        \n",
    "        s = s.lower().strip()\n",
    "        \n",
    "        # 4ft x 2ft\n",
    "        s = s.replace(\" x \",\" \" + XBY + \" \")\n",
    "        s = s.replace(\"*\",\" \" + XBY + \" \")        \n",
    "        s = s.replace(\" by \",\" \" + XBY)\n",
    "        s = pattern_xby_d.sub(\" \" + XBY + \" \\1\", s)\n",
    "        s = pattern_d_xby.sub(\"\\1 \" + XBY + \" \", s)\n",
    "        \n",
    "        # units\n",
    "        s = pattern_inch.sub(r\"\\1 inch \", s)\n",
    "        s = pattern_foot.sub(r\"\\1 foot \", s)\n",
    "        s = pattern_pound.sub(r\"\\1 pound \", s)\n",
    "        s = pattern_sqft.sub(r\"\\1 sqft \", s)\n",
    "        s = pattern_gallons.sub(r\"\\1 gal \", s)\n",
    "        s = pattern_oz.sub(r\"\\1 oz \", s)\n",
    "        s = pattern_cm.sub(r\"\\1 cm \", s)\n",
    "        s = pattern_mm.sub(r\"\\1 mm \", s)\n",
    "        s = pattern_deg.sub(r\"\\1 deg \", s)\n",
    "        s = pattern_volt.sub(r\"\\1 volt \", s)\n",
    "        s = pattern_watt.sub(r\"\\1 watt \", s)\n",
    "        s = pattern_amp.sub(r\"\\1 amp \", s)\n",
    "        s = pattern_kamp.sub(r\"\\1 kamp \", s)\n",
    "        \n",
    "        # some by hand\n",
    "        s = s.replace(\"whirpool\",\"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolga\", \"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolstainless\",\"whirlpool stainless\")\n",
    "        s = s.replace(\"pressure-treated\",\"pressure treated pt\")\n",
    "        \n",
    "        s = ' '.join([x for x in pattern_split.split(s) if x and x not in known_words])\n",
    "        return s\n",
    "    else:\n",
    "        #raise ValueError(\"Type of \" + str(s) + \" is \" + str(type(s)))\n",
    "        #print \"HUY\"\n",
    "        return 'null'\n",
    "    \n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:str_stem(x))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:str_stem(x))\n",
    "df_all['product_description'] = df_all['product_description'].map(lambda x:str_stem(x))\n",
    "df_all['brand'] = df_all['brand'].map(lambda x:str_stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_common_word(str1, str2):\n",
    "    words, cnt = str1.split(), 0\n",
    "    for word in words:\n",
    "        if str2.find(word)>=0:\n",
    "            cnt+=1\n",
    "    return cnt\n",
    "\n",
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# id\tproduct_title\tproduct_uid\trelevance\tsearch_term\tproduct_description\tbrand\n",
    "# id, relevance, search_term, product_title, product_description, (product_uid,) brand  [product_info, attr]\n",
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, hd_searches):\n",
    "        d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand'] + \\\n",
    "        ['tit', 'tit2', 'desc', 'desc2', 'attributes', 'mfgbrand', 'mfgbrand2'] + \\\n",
    "        ['brand_feature'] #['ratio_brand']\n",
    "        #[] #['ratio_title', 'ratio_description', 'ratio_brand']\n",
    "        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n",
    "        return hd_searches\n",
    "\n",
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].apply(str)\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "def fmse(ground_truth, predictions):\n",
    "    return mean_squared_error(ground_truth, predictions)\n",
    "\n",
    "#RMSE  = make_scorer(fmse, greater_is_better=False)\n",
    "RMSE  = make_scorer(fmean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_word(r):\n",
    "    title = r['product_title'].split()\n",
    "    ms = r['tit'].split(\",\")\n",
    "    ic = -1\n",
    "    for m in ms:\n",
    "        for i, t in enumerate(title):\n",
    "            if m == t and i > ic:\n",
    "                ic = i\n",
    "    if ic < 0:\n",
    "        ic = 10\n",
    "    else:\n",
    "        ic = len(title) - ic - 1\n",
    "    return ic #, ms, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_word_match(r):\n",
    "    title = r['product_title'].split()\n",
    "    ms = r['tit'].split(\",\")\n",
    "    if len(title) > 0 and len(ms) > 0 and title[-1] == ms[-1]:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "   return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word, NWORDS):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words, NWORDS): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word, NWORDS):\n",
    "    candidates = known([word], NWORDS) or known(edits1(word), NWORDS) or known_edits2(word, NWORDS) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "already_matched = {}\n",
    "def similar_words(w1, w2):\n",
    "    if (w1, w2) in already_matched:\n",
    "        return already_matched[(w1, w2)]\n",
    "    if abs(len(w1) - len(w2)) > 2:\n",
    "        res = False\n",
    "    elif len(set(w1).difference(set(w2))) > 2 or len(set(w2).difference(set(w1))) > 2 :\n",
    "        res = False\n",
    "    else:\n",
    "        res = w2 == correct(w1, {w2:1})\n",
    "    already_matched[(w1, w2)] = res\n",
    "    return res\n",
    "    \n",
    "def last_word_match_query_last(r):\n",
    "    title = r['product_title'].split(' ')\n",
    "    ms = r['search_term'].split(' ')\n",
    "    if len(title) > 0 and len(ms) > 0 and similar_words(title[-1], ms[-1]):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features Set: 3.31 minutes ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#comment out the lines below use df_all.csv for further grid search testing\n",
    "#if adding features consider any drops on the 'cust_regression_vals' class\n",
    "\n",
    "df_all['product_info'] = df_all['search_term']+\"\\t\"+df_all['product_title'] +\"\\t\"+df_all['product_description']\n",
    "df_all['attr'] = df_all['search_term']+\"\\t\"+df_all['brand']\n",
    "\n",
    "df_all['len_of_query'] = df_all['search_term'].map(lambda x: max(1, len(x.split()))).astype(np.int64)\n",
    "df_all['len_of_title'] = df_all['product_title'].map(lambda x: len(x.split())).astype(np.int64)\n",
    "df_all['len_of_description'] = df_all['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_brand'] = df_all['brand'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "\n",
    "df_all['letters_query'] = df_all['search_term'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_title'] = df_all['product_title'].map(lambda x:len(x)).astype(np.int64)\n",
    "df_all['letters_desc'] = df_all['product_description'].map(lambda x:len(x)).astype(np.int64)\n",
    "df_all['letters_brand'] = df_all['brand'].map(lambda x:len(x)).astype(np.int64)\n",
    "\n",
    "###############################\n",
    "# df_all['query_in_title'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "# df_all['query_in_description'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))\n",
    "\n",
    "# df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "# df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "# df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "\n",
    "df_all['query_in_title'] = df_all['tit'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_description'] = df_all['desc'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_attrs'] = df_all['attributes'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_brand'] = df_all['mfgbrand'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "\n",
    "df_all['letters_query_in_title'] = df_all['tit'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_query_in_description'] = df_all['desc'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_query_in_attrs'] = df_all['attributes'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_query_in_brand'] = df_all['mfgbrand'].map(lambda x: len(x)).astype(np.int64)\n",
    "\n",
    "\n",
    "df_all['query_in_title2'] = df_all['tit2'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_description2'] = df_all['desc2'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_brand2'] = df_all['mfgbrand2'].map(lambda y: len([x for x in y.split(\",\") if x and not re.match(r'^[0-9]+$', x)]))\n",
    "\n",
    "\n",
    "df_all['letters_query_in_title2'] = df_all['tit2'].map(lambda x: len(x)).astype(np.int64)\n",
    "df_all['letters_query_in_description2'] = df_all['desc2'].map(lambda x: len(x)).astype(np.int64)\n",
    "\n",
    "df_all['ratio_letters_query_in_title'] = df_all['letters_query_in_title2']/(df_all['letters_query_in_title'] + 1)\n",
    "df_all['ratio_letters_query_in_descr'] = df_all['letters_query_in_description2']/(df_all['letters_query_in_description'] + 1)\n",
    "\n",
    "\n",
    "df_all['query_in_title_num'] = df_all['tit'].map(lambda y: len([x for x in y.split(\",\") if x and re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_description_num'] = df_all['desc'].map(lambda y: len([x for x in y.split(\",\") if x and re.match(r'^[0-9]+$', x)]))\n",
    "df_all['query_in_attrs_num'] = df_all['attributes'].map(lambda y: len([x for x in y.split(\",\") if x and re.match(r'^[0-9]+$', x)]))\n",
    "\n",
    "\n",
    "\n",
    "#df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "#df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "#df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "###############################\n",
    "\n",
    "\n",
    "df_all['ratio_title'] = df_all['query_in_title']/df_all['len_of_query']\n",
    "df_all['ratio_description'] = df_all['query_in_description']/df_all['len_of_query']\n",
    "\n",
    "# df_all['ratio_title'] = df_all['word_in_title']/df_all['len_of_query']\n",
    "# df_all['ratio_description'] = df_all['word_in_description']/df_all['len_of_query']\n",
    "# df_all['ratio_brand'] = df_all['word_in_brand']/df_all['len_of_brand']\n",
    "\n",
    "\n",
    "\n",
    "df_all['title_match_last_title_word'] = df_all.apply(last_word, axis=1)\n",
    "df_all['title_match_last_word'] = df_all.apply(last_word_match, axis=1)\n",
    "df_all['title_match_last_word_query'] = df_all.apply(last_word_match_query_last, axis=1)\n",
    "\n",
    "df_brand = pd.unique(df_all.brand.ravel())\n",
    "d={}\n",
    "i = 1\n",
    "for s in df_brand:\n",
    "    d[s]=i\n",
    "    i+=1\n",
    "df_all['brand_feature'] = df_all['brand'].map(lambda x: d[x])\n",
    "#df_all['search_term_feature'] = df_all['search_term'].map(lambda x:len(x))\n",
    "\n",
    "#df_all.to_csv('df_all_322_1.csv')\n",
    "#df_all = pd.read_csv('df_all.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "\n",
    "df_train = df_all.iloc[:num_train]\n",
    "df_test = df_all.iloc[num_train:]\n",
    "id_test = df_test['id']\n",
    "y_train = df_train['relevance'].values\n",
    "X_train = df_train[:]\n",
    "X_test = df_test[:]\n",
    "print(\"--- Features Set: %s minutes ---\" % round(((time.time() - start_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_all[1000:1030]\n",
    "#df_all['query_in_title'] + 1\n",
    "#df_all['query_in_title2']/(df_all['query_in_title'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD FROM SAVED\n",
    "#df_all = pd.read_csv('df_all.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "#df_all = pd.read_csv('df_all_322_1.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "\n",
    "df_train = df_all.iloc[:num_train]\n",
    "df_test = df_all.iloc[num_train:]\n",
    "id_test = df_test['id']\n",
    "y_train = df_train['relevance'].values\n",
    "X_train = df_train[:]\n",
    "X_test = df_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_all[300:320][['relevance', 'product_title', 'search_term', 'product_description']]\n",
    "#X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "tsvd = TruncatedSVD(n_components=10, random_state = 2016)\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# dictvect = DictVectorizer()\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohenc = OneHotEncoder()\n",
    "randomForestRegressor = RandomForestRegressor(n_estimators = 500, min_samples_leaf=3, n_jobs = -1, random_state = 5017, verbose = 1)\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "                    transformer_list = [\n",
    "                        ('cst',  cust_regression_vals()),  \n",
    "                    \n",
    "                        ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n",
    "                        ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),\n",
    "                        ('txt3', pipeline.Pipeline([('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf), ('tsvd3', tsvd)])),\n",
    "                        ('txt4', pipeline.Pipeline([('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf), ('tsvd4', tsvd)]))\n",
    "                    \n",
    "#                         ('txt1', pipeline.Pipeline([ ('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf)  ])),\n",
    "#                         ('txt2', pipeline.Pipeline([ ('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf)  ])),\n",
    "#                         ('txt3', pipeline.Pipeline([ ('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf) ])),\n",
    "#                         ('txt4', pipeline.Pipeline([ ('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf) ]))\n",
    "                    \n",
    "#                         ('brandf', pipeline.Pipeline([ ('s5', cust_txt_col(key='brand_feature')), ('ohenc', ohenc)  ])),\n",
    "                        ],\n",
    "                    transformer_weights = {\n",
    "                        'cst': 1.0,\n",
    "                        'txt1': 0.5,\n",
    "                        'txt2': 0.25,\n",
    "                        'txt3': 0.5,\n",
    "                        'txt4': 0.5\n",
    "#                         'brandf': 1.0\n",
    "                        },\n",
    "                n_jobs = -1\n",
    "                ))\n",
    "#         , \n",
    "#         ('rfr', randomForestRegressor)\n",
    "    ])\n",
    "\n",
    "#clf.set_params(rfr__max_features=10, rfr__max_depth=20)\n",
    "#clf.fit(X_train, y_train)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00001000e+05,   1.50000000e+01,   2.00000000e+00, ...,\n",
       "          6.49899457e-05,  -1.09856277e-03,  -6.84634068e-03],\n",
       "       [  1.00001000e+05,   1.50000000e+01,   2.00000000e+00, ...,\n",
       "         -3.87713348e-06,  -1.13283073e-03,  -6.72812595e-03],\n",
       "       [  1.00002000e+05,   3.50000000e+01,   1.00000000e+00, ...,\n",
       "         -5.63536932e-03,   2.92041213e-03,  -5.97269368e-03],\n",
       "       ..., \n",
       "       [  2.06641000e+05,   4.80000000e+01,   6.00000000e+00, ...,\n",
       "         -1.36185457e-06,   2.63435202e-05,   8.18920971e-06],\n",
       "       [  2.06648000e+05,   1.40000000e+01,   3.00000000e+00, ...,\n",
       "         -6.40421081e-16,  -3.94993107e-16,  -1.12821783e-15],\n",
       "       [  2.06650000e+05,   3.30000000e+01,   5.00000000e+00, ...,\n",
       "         -7.15176477e-10,  -7.51147525e-08,   5.51772459e-08]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = clf.fit_transform(X_train)\n",
    "#model.fit(X_train, y_train)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = clf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix( a, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain.save_binary(\"train.buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(b)\n",
    "dtest.save_binary(\"test.buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evallist  = [(dtrain,'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param = {'max_depth':2, \n",
    "         'eta':3, \n",
    "#          'objective':'reg:linear',\n",
    "         'eval_metric':'rmse',\n",
    "         'maximize': False,\n",
    "         'colsample_bytree':1,\n",
    "         'subsample':1,\n",
    "         'nthread':4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_round = 4\n",
    "bst = xgb.train( param, dtrain, num_round, evallist, early_stopping_rounds=15, verbose_eval=2)\n",
    "bst.best_iteration, bst.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>-1.381635</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test-error-mean  test-error-std  train-error-mean  train-error-std\n",
       "0        -1.381635        0.002793         -1.381635         0.002793\n",
       "1        -1.381635        0.002793         -1.381635         0.002793\n",
       "2        -1.381635        0.002793         -1.381635         0.002793\n",
       "3        -1.381635        0.002793         -1.381635         0.002793"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plst = list(param.items())\n",
    "xgb.cv(param, dtrain, num_boost_round=num_round, nfold=2, metrics={'error'}, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reg_alpha',\n",
       " 'colsample_bytree',\n",
       " 'silent',\n",
       " 'colsample_bylevel',\n",
       " 'scale_pos_weight',\n",
       " 'learning_rate',\n",
       " 'missing',\n",
       " 'max_delta_step',\n",
       " 'nthread',\n",
       " 'base_score',\n",
       " 'n_estimators',\n",
       " 'subsample',\n",
       " 'reg_lambda',\n",
       " 'seed',\n",
       " 'min_child_weight',\n",
       " 'objective',\n",
       " 'max_depth',\n",
       " 'gamma']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(param)\n",
    "paramso = {'max_depth': [2], \n",
    "         'learning_rate': [3], \n",
    "         'colsample_bytree':[1],\n",
    "         'subsample':[1]\n",
    "        }\n",
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "model = grid_search.GridSearchCV(estimator = clf, param_grid = paramso, n_jobs = -1, cv = 5, verbose = 20, scoring=RMSE)\n",
    "model.fit(a, y_train)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.27597213,  2.03546667,  2.45794249, ...,  1.62093401,\n",
       "        2.60387492,  2.5656836 ], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  1.5min remaining:  -15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  1.5min remaining:  -15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  1.5min remaining:  -15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  1.9min remaining:  -19.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   20.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   27.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search:\n",
      "{'max_features': 2, 'max_depth': 30}\n",
      "Best CV score:\n",
      "-0.466961553969\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] .......... max_features=2, max_depth=30, score=-0.474227 - 1.5min[CV] .......... max_features=2, max_depth=30, score=-0.461016 - 1.5min[CV] .......... max_features=2, max_depth=30, score=-0.455927 - 1.5min[CV] .......... max_features=2, max_depth=30, score=-0.464262 - 1.5min\n",
      "\n",
      "\n",
      "\n",
      "[CV] max_features=2, max_depth=30 ....................................\n",
      "[CV] .......... max_features=2, max_depth=30, score=-0.479375 -  22.6s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_features': [2], 'max_depth': [30]}\n",
    "model = grid_search.GridSearchCV(estimator = randomForestRegressor, param_grid = param_grid, n_jobs = -1, cv = 5, verbose = 20, scoring=RMSE)\n",
    "model.fit(a, y_train)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  2.3min remaining:  -23.3s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  2.3min remaining:  -23.5s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  2.4min remaining:  -23.8s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   5 | elapsed:  3.1min remaining:  -30.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.1min finished\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.5min finished\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "/usr/local/lib64/python2.7/site-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for name, trans in self.transformer_list)\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   24.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.481643 - 2.2min[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.466586 - 2.3min[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.461499 - 2.2min[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.469350 - 2.2min\n",
      "\n",
      "\n",
      "\n",
      "[CV] rfr__max_features=2, rfr__max_depth=30 ..........................\n",
      "[CV]  rfr__max_features=2, rfr__max_depth=30, score=-0.486730 -  45.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   31.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training: 4.04 minutes ---\n",
      "Best parameters found by grid search:\n",
      "{'rfr__max_features': 2, 'rfr__max_depth': 30}\n",
      "Best CV score:\n",
      "-0.473161739747\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "param_grid = {'rfr__max_features': [2], 'rfr__max_depth': [30]}\n",
    "model = grid_search.GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 5, verbose = 20, scoring=RMSE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Training: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train.ix[3782]\n",
    "inds = pd.isnull(X_train).any(1).nonzero()[0]\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.isfinite(X_train.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.isfinite(X_train).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search:\n",
      "{'rfr__max_features': 2, 'rfr__max_depth': 30}\n",
      "Best CV score:\n",
      "-0.468286123481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    6.0s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission_x.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmi = pd.read_csv('./submission6_50t_50m.csv', encoding=\"ISO-8859-1\")\n",
    "fo = pd.read_csv('./submission_x.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.006440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.045906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.149894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.496521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.222372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance\n",
       "0   1   2.006440\n",
       "1   4   2.045906\n",
       "2   5   2.149894\n",
       "3   6   2.496521\n",
       "4   7   2.222372"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.055402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.163738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.175020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.651792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.345252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance\n",
       "0   1   2.055402\n",
       "1   4   2.163738\n",
       "2   5   2.175020\n",
       "3   6   2.651792\n",
       "4   7   2.345252"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = pd.merge(fo, fmi, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance_x</th>\n",
       "      <th>relevance_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.055402</td>\n",
       "      <td>2.006440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.163738</td>\n",
       "      <td>2.045906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.175020</td>\n",
       "      <td>2.149894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.651792</td>\n",
       "      <td>2.496521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.345252</td>\n",
       "      <td>2.222372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance_x  relevance_y\n",
       "0   1     2.055402     2.006440\n",
       "1   4     2.163738     2.045906\n",
       "2   5     2.175020     2.149894\n",
       "3   6     2.651792     2.496521\n",
       "4   7     2.345252     2.222372"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(r):\n",
    "    return r['relevance_x']*0.1 + r['relevance_y']*0.9\n",
    "\n",
    "y_pred = res.apply(f, axis=1)\n",
    "pd.DataFrame({\"id\": res['id'], \"relevance\": y_pred}).to_csv('submission_x_10_90.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Simpson Strong-Tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>100002.0</td>\n",
       "      <td>BEHR Premium Textured DeckOver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>100003.0</td>\n",
       "      <td>STERLING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>100004.0</td>\n",
       "      <td>Grape Solar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>100005.0</td>\n",
       "      <td>Delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>100006.0</td>\n",
       "      <td>Whirlpool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>100007.0</td>\n",
       "      <td>Lithonia Lighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>100008.0</td>\n",
       "      <td>Teks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>100009.0</td>\n",
       "      <td>House of Fara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>100010.0</td>\n",
       "      <td>Valley View Industries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>100011.0</td>\n",
       "      <td>Toro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>100012.0</td>\n",
       "      <td>Hampton Bay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>100013.0</td>\n",
       "      <td>InSinkErator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>100014.0</td>\n",
       "      <td>Rubbermaid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>100015.0</td>\n",
       "      <td>Backyard X-Scapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>100016.0</td>\n",
       "      <td>Sunjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>100017.0</td>\n",
       "      <td>MD Building Products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>100018.0</td>\n",
       "      <td>Bloem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>100019.0</td>\n",
       "      <td>House of Fara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>100020.0</td>\n",
       "      <td>BAZZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>100021.0</td>\n",
       "      <td>Rain Bird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>100022.0</td>\n",
       "      <td>Samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>100023.0</td>\n",
       "      <td>Quikrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>100024.0</td>\n",
       "      <td>Goal Zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>100025.0</td>\n",
       "      <td>Stal wart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>100026.0</td>\n",
       "      <td>Nantucket Pavers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>100027.0</td>\n",
       "      <td>UltraTouch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>100028.0</td>\n",
       "      <td>Backyard X-Scapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>100029.0</td>\n",
       "      <td>DecoArt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>100030.0</td>\n",
       "      <td>Unbranded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044058</th>\n",
       "      <td>224396.0</td>\n",
       "      <td>Oatey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044083</th>\n",
       "      <td>224397.0</td>\n",
       "      <td>Amflo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044106</th>\n",
       "      <td>224398.0</td>\n",
       "      <td>DEWALT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044136</th>\n",
       "      <td>224399.0</td>\n",
       "      <td>Home Fashion Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044154</th>\n",
       "      <td>224400.0</td>\n",
       "      <td>Eaton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044178</th>\n",
       "      <td>224401.0</td>\n",
       "      <td>Crown Bolt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044192</th>\n",
       "      <td>224402.0</td>\n",
       "      <td>Dickies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044217</th>\n",
       "      <td>224403.0</td>\n",
       "      <td>Patio Living Concepts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044244</th>\n",
       "      <td>224404.0</td>\n",
       "      <td>Speakman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044267</th>\n",
       "      <td>224405.0</td>\n",
       "      <td>Plantronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044297</th>\n",
       "      <td>224406.0</td>\n",
       "      <td>Glomar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044335</th>\n",
       "      <td>224407.0</td>\n",
       "      <td>Kwikset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044351</th>\n",
       "      <td>224408.0</td>\n",
       "      <td>Advanced Drainage Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044379</th>\n",
       "      <td>224409.0</td>\n",
       "      <td>Magnavox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044399</th>\n",
       "      <td>224410.0</td>\n",
       "      <td>Ekena Millwork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044427</th>\n",
       "      <td>224411.0</td>\n",
       "      <td>Minwax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044450</th>\n",
       "      <td>224412.0</td>\n",
       "      <td>Southern Living Plant Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044493</th>\n",
       "      <td>224413.0</td>\n",
       "      <td>GE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044523</th>\n",
       "      <td>224414.0</td>\n",
       "      <td>Southwire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044555</th>\n",
       "      <td>224415.0</td>\n",
       "      <td>Generac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044574</th>\n",
       "      <td>224416.0</td>\n",
       "      <td>KOHLER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044592</th>\n",
       "      <td>224417.0</td>\n",
       "      <td>Casablanca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044607</th>\n",
       "      <td>224419.0</td>\n",
       "      <td>Everbilt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044629</th>\n",
       "      <td>224420.0</td>\n",
       "      <td>Crown Bolt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044665</th>\n",
       "      <td>224421.0</td>\n",
       "      <td>Universal Tubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044702</th>\n",
       "      <td>224422.0</td>\n",
       "      <td>Everbilt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044734</th>\n",
       "      <td>224423.0</td>\n",
       "      <td>Coastal Shower Doors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044747</th>\n",
       "      <td>224424.0</td>\n",
       "      <td>stufurhome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044774</th>\n",
       "      <td>224425.0</td>\n",
       "      <td>Home Decorators Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044795</th>\n",
       "      <td>224428.0</td>\n",
       "      <td>Bosch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         product_uid                             brand\n",
       "9           100001.0                Simpson Strong-Tie\n",
       "37          100002.0    BEHR Premium Textured DeckOver\n",
       "69          100003.0                          STERLING\n",
       "93          100004.0                       Grape Solar\n",
       "122         100005.0                             Delta\n",
       "163         100006.0                         Whirlpool\n",
       "204         100007.0                 Lithonia Lighting\n",
       "236         100008.0                              Teks\n",
       "256         100009.0                     House of Fara\n",
       "283         100010.0            Valley View Industries\n",
       "318         100011.0                              Toro\n",
       "363         100012.0                       Hampton Bay\n",
       "389         100013.0                      InSinkErator\n",
       "405         100014.0                        Rubbermaid\n",
       "423         100015.0                 Backyard X-Scapes\n",
       "441         100016.0                            Sunjoy\n",
       "459         100017.0              MD Building Products\n",
       "477         100018.0                             Bloem\n",
       "492         100019.0                     House of Fara\n",
       "516         100020.0                              BAZZ\n",
       "540         100021.0                         Rain Bird\n",
       "587         100022.0                           Samsung\n",
       "616         100023.0                          Quikrete\n",
       "629         100024.0                         Goal Zero\n",
       "641         100025.0                         Stal wart\n",
       "661         100026.0                  Nantucket Pavers\n",
       "680         100027.0                        UltraTouch\n",
       "695         100028.0                 Backyard X-Scapes\n",
       "709         100029.0                           DecoArt\n",
       "729         100030.0                         Unbranded\n",
       "...              ...                               ...\n",
       "2044058     224396.0                             Oatey\n",
       "2044083     224397.0                             Amflo\n",
       "2044106     224398.0                            DEWALT\n",
       "2044136     224399.0         Home Fashion Technologies\n",
       "2044154     224400.0                             Eaton\n",
       "2044178     224401.0                        Crown Bolt\n",
       "2044192     224402.0                           Dickies\n",
       "2044217     224403.0             Patio Living Concepts\n",
       "2044244     224404.0                          Speakman\n",
       "2044267     224405.0                       Plantronics\n",
       "2044297     224406.0                            Glomar\n",
       "2044335     224407.0                           Kwikset\n",
       "2044351     224408.0         Advanced Drainage Systems\n",
       "2044379     224409.0                          Magnavox\n",
       "2044399     224410.0                    Ekena Millwork\n",
       "2044427     224411.0                            Minwax\n",
       "2044450     224412.0  Southern Living Plant Collection\n",
       "2044493     224413.0                                GE\n",
       "2044523     224414.0                         Southwire\n",
       "2044555     224415.0                           Generac\n",
       "2044574     224416.0                            KOHLER\n",
       "2044592     224417.0                        Casablanca\n",
       "2044607     224419.0                          Everbilt\n",
       "2044629     224420.0                        Crown Bolt\n",
       "2044665     224421.0                    Universal Tubs\n",
       "2044702     224422.0                          Everbilt\n",
       "2044734     224423.0              Coastal Shower Doors\n",
       "2044747     224424.0                        stufurhome\n",
       "2044774     224425.0        Home Decorators Collection\n",
       "2044795     224428.0                             Bosch\n",
       "\n",
       "[86250 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fmi = pd.read_csv('./submission10_60G_40XG.csv.gz', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.048145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.071236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.223909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.612672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.274974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance\n",
       "0   1   2.048145\n",
       "1   4   2.071236\n",
       "2   5   2.223909\n",
       "3   6   2.612672\n",
       "4   7   2.274974"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def froi(x):\n",
    "    if x > 2.85:\n",
    "        return 3.0\n",
    "    elif x > 2.55:\n",
    "        return 2.66\n",
    "    elif x < 1.15:\n",
    "        return 1.0\n",
    "    elif x < 1.45:\n",
    "        return 1.33\n",
    "    elif x < 2.15 and x > 1.85:\n",
    "        return 2.0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmi['relevance'] = fmi['relevance'].apply(froi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.223909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.274974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance\n",
       "0   1   2.000000\n",
       "1   4   2.000000\n",
       "2   5   2.223909\n",
       "3   6   2.660000\n",
       "4   7   2.274974"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmi.to_csv('submission_collapse_some_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google_spell\n",
    "from google_spell import spell_check_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'congolium' in spell_check_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congoleum'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_dict['congolium']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
