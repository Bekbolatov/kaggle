{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://vene.ro/blog/word-movers-distance-in-python.html\n",
    "http://sujitpal.blogspot.com/2015/09/sentence-similarity-using-word2vec-and.html\n",
    "https://github.com/wmayner/pyemd\n",
    "http://www.ariel.ac.il/sites/ofirpele/fastemd/\n",
    "http://nbviewer.jupyter.org/github/vene/vene.github.io/blob/pelican/content/blog/word-movers-distance-in-python.ipynb\n",
    "https://github.com/mkusner/wmd/blob/master/compute_rwmd.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = '/Users/rbekbolatov/data/Word2Vec/%s'\n",
    "if not os.path.exists(loc % \"embed.dat\"):\n",
    "    print(\"Caching word embeddings in memmapped format...\")\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    wv = Word2Vec.load_word2vec_format(loc % \"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "    fp = np.memmap(loc % \"embed.dat\", dtype=np.double, mode='w+', shape=wv.syn0norm.shape)\n",
    "    fp[:] = wv.syn0norm[:]\n",
    "    with open(loc % \"embed.vocab\", \"w\") as f:\n",
    "        for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "            print >> f, w.encode('utf-8')\n",
    "            #print(w.encode('utf-8'), file=f)\n",
    "    del fp, wv\n",
    "\n",
    "W = np.memmap(loc % \"embed.dat\", dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "with open(loc % \"embed.vocab\") as f:\n",
    "    vocab_list = map(str.strip, f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict = {w: k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = \"Obama speaks to the media in Illinois\"\n",
    "d2 = \"The President addresses the press in Chicago\"\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "print(\"Features:\",  \", \".join(vect.get_feature_names()))\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "v_1, v_2 = vect.transform([d1, d2])\n",
    "v_1 = v_1.toarray().ravel()\n",
    "v_2 = v_2.toarray().ravel()\n",
    "print(v_1, v_2)\n",
    "print(\"cosine(doc_1, doc_2) = {:.2f}\".format(cosine(v_1, v_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "W_ = W[[vocab_dict[w] for w in vect.get_feature_names()]]\n",
    "D_ = euclidean_distances(W_)\n",
    "print(\"d(addresses, speaks) = {:.2f}\".format(D_[0, 7]))\n",
    "print(\"d(addresses, chicago) = {:.2f}\".format(D_[0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "\n",
    "# pyemd needs double precision input\n",
    "v_1 = v_1.astype(np.double)\n",
    "v_2 = v_2.astype(np.double)\n",
    "v_1 /= v_1.sum()\n",
    "v_2 /= v_2.sum()\n",
    "D_ = D_.astype(np.double)\n",
    "D_ /= D_.max()  # just for comparison purposes\n",
    "print(\"d(doc_1, doc_2) = {:.2f}\".format(emd(v_1, v_2, D_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_WMD = 0.0\n",
    "def wmd(d1, d2):\n",
    "    vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "    v_1, v_2 = vect.transform([d1, d2])\n",
    "    v_1 = v_1.toarray().ravel()\n",
    "    v_2 = v_2.toarray().ravel()\n",
    "    print(v_1, v_2)\n",
    "    print(\"cosine(doc_1, doc_2) = {:.2f}\".format(cosine(v_1, v_2)))\n",
    "    W_ = W[[vocab_dict[w] for w in vect.get_feature_names() if w in vocab_dict]]\n",
    "    if len(W_) < 1:\n",
    "        return DEFAULT_WMD\n",
    "    D_ = euclidean_distances(W_)\n",
    "    print(D_)\n",
    "    v_1 = v_1.astype(np.double)\n",
    "    v_2 = v_2.astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max()  # just for comparison purposes\n",
    "    return emd(v_1, v_2, D_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = '/Users/rbekbolatov/data/Word2Vec/%s'\n",
    "loc = '/home/ec2-user/data/word2vec/%s'\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "wv = Word2Vec.load_word2vec_format(loc % \"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from pyemd import emd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "DEFAULT_WMD = 0.0\n",
    "def wmd(d1, d2):\n",
    "    vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "    v_1, v_2 = vect.transform([d1, d2])\n",
    "    v_1 = v_1.toarray().ravel()\n",
    "    v_2 = v_2.toarray().ravel()\n",
    "    #print(v_1, v_2)\n",
    "    print(\"cosine(doc_1, doc_2) = {:.2f}\".format(cosine(v_1, v_2)))\n",
    "    indic = [w in wv for w in vect.get_feature_names()]   \n",
    "    \n",
    "    v_1 = np.array([v for b,v in zip(indic, v_1) if b])\n",
    "    v_2 = np.array([v for b,v in zip(indic, v_2) if b]) \n",
    "    W_ = [wv[w] for w in vect.get_feature_names() if w in wv]\n",
    "    \n",
    "    #print len(v_1), len(v_2), len(W_)\n",
    "    # here sometimes there are words that are not in wv, \n",
    "    # then, len(v_1) can be > len(W_) and segfault\n",
    "    if len(W_) < 1:\n",
    "        return DEFAULT_WMD\n",
    "    if v_1.sum() < 0.000001:\n",
    "        return DEFAULT_WMD\n",
    "    if v_2.sum() < 0.000001:\n",
    "        return DEFAULT_WMD\n",
    "\n",
    "    D_ = euclidean_distances(W_)\n",
    "    #print(D_)\n",
    "    v_1 = v_1.astype(np.double)\n",
    "    v_2 = v_2.astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max()  # just for comparison purposes\n",
    "    return emd(v_1, v_2, D_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = 'Obama speaks to the media in Illinois'\n",
    "d2 = 'The president greets the press in Chicago'\n",
    "wmd(d1, d2)\n",
    "\n",
    "\n",
    "d1 = 'Simpson Strong-Tie 12-Gauge Angle'\n",
    "d2 = 'angle bracket'\n",
    "wmd(d1, d2)\n",
    "\n",
    "d1 = 'Simpson Strong-Tie 12-Gauge Angle'\n",
    "d2 = 'convection otr'\n",
    "wmd(d1, d2)\n",
    "\n",
    "d1 = 'Simpson Strong-Tie 12-Gauge Angle'\n",
    "d2 = 'microwave over stove'\n",
    "wmd(d1, d2)\n",
    "\n",
    "d1 = 'Whirlpool 1.9 cu. ft. Over the Range Convection Microwave in Stainless Steel with Sensor Cooking'\n",
    "d2 = 'angle bracket'\n",
    "wmd(d1, d2)\n",
    "\n",
    "d1 = 'Whirlpool 1.9 cu. ft. Over the Range Convection Microwave in Stainless Steel with Sensor Cooking'\n",
    "d2 = 'convection otr'\n",
    "wmd(d1, d2)\n",
    "\n",
    "\n",
    "d1 = 'Whirlpool 1.9 cu. ft. Over the Range Convection Microwave in Stainless Steel with Sensor Cooking'\n",
    "d2 = 'microwave over stove'\n",
    "wmd(d1, d2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, min_count=2, seed=42, workers=1)\n",
    "\n",
    "\n",
    "model = wv\n",
    "wv['book']\n",
    "\n",
    "sentence1 = ['human', 'interface', 'computer']\n",
    "sentence2 = ['survey', 'user', 'computer', 'system', 'response', 'time']\n",
    "model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'.split()\n",
    "sentence_president = 'The president greets the press in Chicago'.split()\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "\n",
    "\n",
    "sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
    "sentence_president = [w for w in sentence_president if w not in stopwords]\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d1s = ['Simpson Strong-Tie 12-Gauge Angle', \n",
    "      'BEHR Premium Textured DeckOver 1-gal. #SC-141 Tugboat Wood and Concrete Coating', \n",
    "      'Delta Vero 1-Handle Shower Only Faucet Trim Kit in Chrome (Valve Not Included)']\n",
    "d2s = ['angle bracket', 'deck over', 'convection otr']\n",
    "\n",
    "for d1 in d1s:\n",
    "    for d2 in d2s:\n",
    "        print \"%s:  %s\" % (d1, d2)\n",
    "        print model.wmdistance(d1, d2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = '/home/ec2-user/data/hd/unpacked/%s'\n",
    "\n",
    "train = pd.read_csv(loc % 'train.csv')\n",
    "test = pd.read_csv(loc % 'test.csv')\n",
    "from google_spell import correct_spelling\n",
    "\n",
    "def f(row):\n",
    "    d1 = correct_spelling(row['search_term']).split()\n",
    "    d2 = row['product_title'].split()\n",
    "    if (d1 == d2):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return model.wmdistance(d1, d2)\n",
    "\n",
    "\n",
    "\n",
    "tt = train.sample(10000)\n",
    "tt['w'] = tt.apply(f, axis=1)\n",
    "#w_train = train.apply(f, axis=1)\n",
    "\n",
    "tt[(tt.relevance == 3.0) & (tt.w != float('inf'))]['w'].mean()\n",
    "1.2317953205871341\n",
    "\n",
    "tt[(tt.relevance == 1.0) & (tt.w != float('inf'))]['w'].mean()\n",
    "1.2805398115662407\n",
    "\n",
    "tt[(tt.relevance == 2.0) & (tt.w != float('inf'))]['w'].mean()\n",
    "1.2465750573587313\n",
    "    \n",
    "\n",
    "train['wmd_goognews'] = train.apply(f, axis=1)\n",
    "print \"good\"\n",
    "test['wmd_goognews'] = test.apply(f, axis=1)\n",
    "print \"good\"\n",
    "\n",
    "\n",
    "train['w'] = train['wmd_goognews']\n",
    "test['w'] = test['wmd_goognews']\n",
    "\n",
    "\n",
    "tt = train\n",
    "\n",
    "\n",
    "train[['id', 'wmd_goognews']].to_csv('wmd_gn_train.csv', index=False)\n",
    "test[['id', 'wmd_goognews']].to_csv('wmd_gn_test.csv', index=False)\n",
    "    \n",
    "\n",
    "    \n",
    "def f_clean_query(row):\n",
    "    return correct_spelling(row['search_term'])\n",
    "    \n",
    "train['cleaned_query'] = train.apply(f_clean_query, axis=1)\n",
    "test['cleaned_query'] = test.apply(f_clean_query, axis=1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
