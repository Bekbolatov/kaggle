{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn import pipeline, model_selection\n",
    "from sklearn import pipeline, grid_search\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = '/home/ec2-user/data/hd/features/%s'\n",
    "\n",
    "queries = pd.read_pickle(loc % 'FEATURES_WITH_TEXT_1.data')\n",
    "\n",
    "idx_train = pd.read_pickle(loc % 'LABELS_TRAIN.df')\n",
    "idx_test = pd.read_pickle(loc % 'LABELS_TEST.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDX_TR, IDX_TE = train_test_split(idx_train.index, test_size=0.20, random_state=117)\n",
    "\n",
    "IDX_TRAIN = idx_train.loc[IDX_TR]\n",
    "\n",
    "IDX_TEST = idx_train.loc[IDX_TE]\n",
    "IDX_TEST['orig_rel'] = IDX_TEST['relevance']\n",
    "IDX_TEST['relevance'] = -1\n",
    "\n",
    "idx_test['relevance'] = -1\n",
    "\n",
    "known_labels = pd.concat([IDX_TRAIN, IDX_TEST, idx_test]).join(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "has_digit = re.compile('([0-9]|units|xby)')\n",
    "\n",
    "def calculate_word_pairs(a1, a2):\n",
    "    \n",
    "    word_matches = defaultdict(lambda : defaultdict(lambda : defaultdict(lambda: 0)))\n",
    "    word_matches_score_raw = defaultdict(lambda : defaultdict(lambda : defaultdict(lambda: 0)))\n",
    "    word_matches_score_offset = defaultdict(lambda : defaultdict(lambda : defaultdict(lambda: 0)))\n",
    "    word_counts = defaultdict(lambda : defaultdict(lambda : 0))\n",
    "    word_counts_score = defaultdict(lambda : defaultdict(lambda : 0))\n",
    "    \n",
    "    def f(r):\n",
    "        ID = r['id']\n",
    "        query = r[a1]\n",
    "        if not query:\n",
    "            query = \"NOWORDS\"\n",
    "        qs = [q for q in query.split() if not has_digit.search(q)]\n",
    "\n",
    "        title = r[a2]\n",
    "        if not title:\n",
    "            title = \"NOWORDS\"\n",
    "        ts = [t for t in title.split() if not has_digit.search(t)]\n",
    "\n",
    "        score = r['relevance']\n",
    "        for q in qs:\n",
    "            c = 0\n",
    "            c_score = 0\n",
    "            for t in ts:\n",
    "                word_matches[ID][q][t] += 1\n",
    "                if score > 0:\n",
    "                    word_matches_score_raw[ID][q][t] += score\n",
    "                    word_matches_score_offset[ID][q][t] += (score - 2.38)\n",
    "                    c_score += 1\n",
    "                c += 1\n",
    "            word_counts[ID][q] += c\n",
    "            word_counts_score[ID][q] += c_score\n",
    "    _ = known_labels.reset_index().apply(f, axis=1)\n",
    "    \n",
    "    return (word_matches, word_matches_score_raw, word_matches_score_offset, word_counts, word_counts_score)\n",
    "    \n",
    "word_matches, word_matches_score_raw, word_matches_score_offset, word_counts, word_counts_score = calculate_word_pairs('query', 'product_title')\n",
    "word_matches2, word_matches_score_raw2, word_matches_score_offset2, word_counts2, word_counts_score2 = calculate_word_pairs('product_title', 'query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_dict(given_word_matches):\n",
    "    total_wm = defaultdict(lambda : defaultdict(lambda: 0))\n",
    "    for ID, wm in given_word_matches.items():\n",
    "        for q, tc in wm.items():\n",
    "            for t, c in tc.items():\n",
    "                total_wm[q][t] += c\n",
    "    total_wm = { q: { t: c for t,c in tc.items() } for q,tc in total_wm.items()}\n",
    "    return total_wm\n",
    "\n",
    "def get_train_counts(given_word_count):\n",
    "    total_wc = defaultdict(lambda : 0)\n",
    "    for ID, wm in given_word_count.items():\n",
    "        for q, c in wm.items():\n",
    "            total_wc[q] += c\n",
    "    total_wc = { t: c for t,c in total_wc.items()}\n",
    "    return total_wc\n",
    "\n",
    "train_word_matches = get_train_dict(word_matches)\n",
    "train_word_matches_score_raw = get_train_dict(word_matches_score_raw)\n",
    "train_word_matches_score_offset = get_train_dict(word_matches_score_offset)\n",
    "train_word_counts = get_train_counts(word_counts)\n",
    "train_word_counts_score = get_train_counts(word_counts_score)\n",
    "\n",
    "train_word_matches2 = get_train_dict(word_matches2)\n",
    "train_word_matches_score_raw2 = get_train_dict(word_matches_score_raw2)\n",
    "train_word_matches_score_offset2 = get_train_dict(word_matches_score_offset2)\n",
    "train_word_counts2 = get_train_counts(word_counts2)\n",
    "train_word_counts_score2 = get_train_counts(word_counts_score2)\n",
    "\n",
    "\n",
    "#sorted([(v, k) for k,v in train_word_matches_score_offset['pt'].items()], reverse=True)[0:12], \n",
    "#sorted([(v, k) for k,v in train_word_matches_score_offset2['showerhead'].items()], reverse=True)[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trimmed_dict(zad, d, zawc, wc, no_id, qs):\n",
    "    \n",
    "    #ad = { q: v.copy() for q, v in zad.items() }\n",
    "    ad = {}\n",
    "    awc = zawc.copy()\n",
    "        \n",
    "    ded = d.get(no_id, None)\n",
    "    dewc = wc.get(no_id, None)\n",
    "    \n",
    "    for q in qs:\n",
    "        adq = zad.get(q, None)\n",
    "        if adq:\n",
    "            ad[q] = zad[q].copy()\n",
    "        else:\n",
    "            ad[q] = {}\n",
    "        \n",
    "    if ded:\n",
    "        for q, tc in ded.items():  \n",
    "            for t, c in tc.items():\n",
    "                ad[q][t] -= c\n",
    "            awc[q] -= dewc[q]\n",
    "            \n",
    "    for q, tc in ad.items():\n",
    "        for t, c in ad[q].items():\n",
    "            denom = awc.get(q, 0) + 1.0\n",
    "            a = ad[q][t]\n",
    "            if a < 0.0000001:\n",
    "                del ad[q][t]\n",
    "            else:\n",
    "                ad[q][t] = a*1.0/denom\n",
    "        if not ad[q]:\n",
    "            del ad[q]\n",
    "    return ad\n",
    "\n",
    "def get_features_no_id(r):\n",
    "    ID = r['id']\n",
    "    \n",
    "    query = r['query']\n",
    "    if not query:\n",
    "        query = \"NOWORDS\"\n",
    "    qs = [q for q in query.split() if not has_digit.search(q)]\n",
    "\n",
    "    title = r['product_title']\n",
    "    if not title:\n",
    "        title = \"NOWORDS\"\n",
    "    ts = [t for t in title.split() if not has_digit.search(t)]\n",
    "\n",
    "    myd1 = trimmed_dict(train_word_matches, word_matches, train_word_counts, word_counts, ID, qs)\n",
    "    myd2 = trimmed_dict(train_word_matches_score_raw, word_matches_score_raw, train_word_counts_score, word_counts_score, ID, qs)\n",
    "    myd3 = trimmed_dict(train_word_matches_score_offset, word_matches_score_offset, train_word_counts_score, word_counts_score, ID, qs)\n",
    "    myd4 = trimmed_dict(train_word_matches2, word_matches2, train_word_counts2, word_counts2, ID, ts)\n",
    "    myd5 = trimmed_dict(train_word_matches_score_raw2, word_matches_score_raw2, train_word_counts_score2, word_counts_score2, ID, ts)\n",
    "    myd6 = trimmed_dict(train_word_matches_score_offset2, word_matches_score_offset2, train_word_counts_score2, word_counts_score2, ID, ts)\n",
    "    \n",
    "\n",
    "\n",
    "    a1, a2, a3 = 0.0, 0.0, 0.0\n",
    "    b1, b2, b3 = 0.0, 0.0, 0.0\n",
    "    for q in qs:\n",
    "        qt1 = myd1.get(q, None)\n",
    "        qt2 = myd2.get(q, None)\n",
    "        qt3 = myd3.get(q, None)\n",
    "        for t in ts:\n",
    "            if qt1:\n",
    "                a1 += qt1.get(t, 0)\n",
    "            if qt2:\n",
    "                a2 += qt2.get(t, 0)\n",
    "            if qt3:\n",
    "                a3 += qt3.get(t, 0)\n",
    "    for t in ts:\n",
    "        tq1 = myd4.get(t, None)\n",
    "        tq2 = myd5.get(t, None)\n",
    "        tq3 = myd6.get(t, None)\n",
    "        for q in qs:\n",
    "            if tq1:\n",
    "                b1 += tq1.get(q, 0)\n",
    "            if tq2:\n",
    "                b2 += tq2.get(q, 0)\n",
    "            if tq3:\n",
    "                b3 += tq3.get(q, 0)\n",
    "    a1 /= (len(qs) + 1)\n",
    "    a2 /= (len(qs) + 1)\n",
    "    a3 /= (len(qs) + 1)\n",
    "    b1 /= (len(ts) + 1)\n",
    "    b2 /= (len(ts) + 1)\n",
    "    b3 /= (len(ts) + 1)        \n",
    "    return pd.Series([ID, a1, a2, a3, b1, b2, b3])\n",
    "\n",
    "#myd1 = trimmed_dict(train_word_matches, word_matches, train_word_counts, word_counts, 214616)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "woqta = known_labels.reset_index().apply(get_features_no_id, axis=1)\n",
    "woqta.columns = ['id', 'woqt1', 'woqt2', 'woqt3', 'woqt4', 'woqt5', 'woqt6']\n",
    "woqta['id'] = woqta['id'].astype(int)\n",
    "woqta.set_index('id', inplace=True)\n",
    "\n",
    "WOQTA_TRAIN = idx_train.join(woqta)\n",
    "WOQTA_TEST = idx_test.join(woqta)\n",
    "\n",
    "WOQTA_TRAIN.to_pickle('WOQTAL_TRAIN_117')\n",
    "WOQTA_TEST.to_pickle('WOQTAL_TEST_117')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
