{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn import pipeline, model_selection\n",
    "from sklearn import pipeline, grid_search\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "random.seed(2017)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import data_load\n",
    "#reload(data_load)\n",
    "#data_load.clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = data_load.get_clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['attributes_len'] = products['attributes'].map(lambda ss: len(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['brand'] = products['attributes'].map(lambda kvs: kvs.get('mfg brand name', 'none'))\n",
    "def b1g(s):\n",
    "    a = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    if s == 'none':\n",
    "        a[0] = 1.0\n",
    "    elif s == 'unbranded':\n",
    "        a[1] = 1.0\n",
    "    elif s == 'hampton bay':\n",
    "        a[2] = 1.0\n",
    "    elif s == 'kohler':\n",
    "        a[3] = 1.0\n",
    "    elif s == 'everbilt':\n",
    "        a[4] = 1.0\n",
    "    elif s == 'home decorators collection':\n",
    "        a[5] = 1.0\n",
    "    elif s == 'ge':\n",
    "        a[6] = 1.0\n",
    "    return a\n",
    "\n",
    "df_brand_features = products['brand'].apply(lambda s: pd.Series(b1g(s)))\n",
    "df_brand_features.columns = ['brand_none', 'brand_unbranded', 'brand_hampton', 'brand_kohler', 'brand_ever', 'brand_home', 'brand_ge']\n",
    "products = pd.concat([products, df_brand_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1g(q):\n",
    "    if 'sxdli' in q:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "products['corrected'] = products['queries'].map(lambda ss: { k:f1g(s) for k,s in ss.items() })\n",
    "\n",
    "def f1(q):\n",
    "    if 'sxdli' in q:\n",
    "        return q[:-6]\n",
    "    else:\n",
    "        return q\n",
    "products['queries'] = products['queries'].map(lambda ss: { k:f1(s) for k,s in ss.items() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# length in words\n",
    "products['queries_len'] = products['queries'].map(lambda kvs: {k:    max(1, len(v.split()))      for k,v in kvs.items()})\n",
    "products['product_title_len'] = products['product_title'].map(lambda x: len(x.split()))\n",
    "products['product_description_len'] = products['product_description'].map(lambda x:len(x.split()))\n",
    "products['brand_len'] = products['brand'].map(lambda x:len(x.split()))\n",
    "# length in words, that have no digits\n",
    "no_number = re.compile(r\"[0-9]\")\n",
    "products['queries_wlen'] = products['queries'].map(lambda kvs: {k:    max(1, len([x for x in v.split() if not no_number.search(x)]))      for k,v in kvs.items()})\n",
    "products['product_title_wlen'] = products['product_title'].map(lambda x: len([x for x in x.split() if not no_number.search(x)]))\n",
    "products['product_description_wlen'] = products['product_description'].map(lambda x:len([x for x in x.split() if not no_number.search(x)]))\n",
    "products['brand_wlen'] = products['brand'].map(lambda x:len([x for x in x.split() if not no_number.search(x)]))\n",
    "# length in letters\n",
    "products['queries_let'] = products['queries'].map(lambda kvs: {k:    max(1, len(v))      for k,v in kvs.items()})\n",
    "products['product_title_let'] = products['product_title'].map(lambda x: len(x))\n",
    "products['product_description_let'] = products['product_description'].map(lambda x:len(x))\n",
    "products['brand_let'] = products['brand'].map(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "has_digit = re.compile(r\"[0-9]\")\n",
    "def str_common_word(str1, str2):\n",
    "    words, cnt, wcnt = str1.split(), 0, 0\n",
    "    matched = set()\n",
    "    wmatched = set()\n",
    "    for word in words:\n",
    "        if len(word) > 1 and str2.find(word)>=0:\n",
    "            cnt+=1\n",
    "            matched.add(word)\n",
    "            if not has_digit.search(word):\n",
    "                wcnt +=1\n",
    "                wmatched.add(word)\n",
    "    matched_len = sum([len(w) for w in matched])\n",
    "    wmatched_len = sum([len(w) for w in wmatched])\n",
    "    leftest = 15\n",
    "    if wcnt > 0:\n",
    "        other_words = str2.split()\n",
    "        other_words.reverse()\n",
    "        for i, w in enumerate(other_words):\n",
    "            if words[-1] == w:\n",
    "                leftest = i \n",
    "                break\n",
    "    return [cnt, cnt*1.0/max(1.0, len(words)), matched_len, wcnt, wmatched_len, leftest]\n",
    "\n",
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    total_len = 0\n",
    "    if len(str1) < 1:\n",
    "        return [cnt, total_len]\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return [cnt, total_len]\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "            total_len += len(str1)\n",
    "    return [cnt, total_len]\n",
    "\n",
    "def bigram_match(str1, str2):\n",
    "    words, intext, cnt = str1.split(), str2.split(), 0\n",
    "    bi1 = zip(words, words[1:])\n",
    "    bi2 = set(zip(intext, intext[1:]))\n",
    "    matched = set()\n",
    "    for x in bi1:\n",
    "        if x in bi2:\n",
    "            cnt+=1\n",
    "            matched.add(x)\n",
    "    matched_len = sum([len(w[0]) + len(w[1]) for w in matched])        \n",
    "    return [cnt, matched_len]\n",
    "\n",
    "def f_query_in(r):\n",
    "    tit = r['product_title']\n",
    "    desc = r['product_description']\n",
    "    attrs = ' '.join([ k + ' ' + v for k,v in r['attributes'].items()])\n",
    "    brand = r['brand']\n",
    "    els = [tit, desc, attrs, brand]\n",
    "    qs = r['queries']\n",
    "    res = { k: [y    for el in els for x in [str_whole_word(v, el, 0) + str_common_word(v, el) + bigram_match(v, el)] for y in x] for k,v in qs.items()}\n",
    "    return res\n",
    "\n",
    "products['query_in_product_features'] = products.apply(f_query_in, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "query_features = ['corrected', 'queries_len', 'queries_let', 'queries_wlen', 'query_in_product_features']\n",
    "\n",
    "def combine_feats(r):\n",
    "    feats = defaultdict(list)\n",
    "    for c in query_features:\n",
    "        ds = r[c]\n",
    "        for k,v in ds.items():\n",
    "            if type(v) == list:\n",
    "                feats[k] += v\n",
    "            else:\n",
    "                feats[k] += [v]\n",
    "    return feats\n",
    "\n",
    "combined_query_features = products[query_features].apply(combine_feats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products['combined_query_features'] = combined_query_features\n",
    "products.drop(query_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "direct_features = [\n",
    "\"brand_none\",\n",
    "\"brand_unbranded\",\n",
    "\"brand_hampton\",\n",
    "\"brand_kohler\",\n",
    "\"brand_ever\",\n",
    "\"brand_home\",\n",
    "\"brand_ge\",\n",
    "\"attributes_len\",\n",
    "\"product_title_len\",\n",
    "\"product_description_len\",\n",
    "\"brand_len\",\n",
    "\"product_title_let\",\n",
    "\"product_description_let\",\n",
    "\"brand_let\",\n",
    "\"product_title_wlen\",\n",
    "\"product_description_wlen\",\n",
    "\"brand_wlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.to_pickle(products, 'RAW_FEATURES')\n",
    "#products = pd.read_pickle('RAW_FEATURES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_columns = ['product_uid', 'product_title', 'product_description', 'brand']\n",
    "def explode(r, rs):\n",
    "    attrs = ' '.join([k + ' ' + v for k,v in r['attributes'].items()])\n",
    "    qs = r['queries']\n",
    "    for i,q in qs.items():\n",
    "        new_row = [i, r['queries'][i]]\n",
    "        new_row += [r[f] for f in product_columns]\n",
    "        new_row += [attrs]\n",
    "        new_row += [r[f] for f in direct_features]\n",
    "        new_row += r['combined_query_features'][i]\n",
    "        rs.append(new_row)\n",
    "        \n",
    "rows = []    \n",
    "_ = products.reset_index().apply(lambda row: explode(row, rows), axis=1)\n",
    "df_combined_query_features = pd.DataFrame(rows)\n",
    "df_combined_query_features.columns = ['id', 'query'] + product_columns + ['attrs'] + direct_features + ['c%d' % i for i in range(44)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries = df_combined_query_features\n",
    "queries.set_index('id', inplace=True)\n",
    "pd.to_pickle(queries, 'FEATURES_WITH_TEXT_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_brand = pd.unique(products.brand.ravel())\n",
    "d={}\n",
    "i = 1\n",
    "for s in df_brand:\n",
    "    d[s]=i\n",
    "    i+=1\n",
    "products['brand_feature'] = products['brand'].map(lambda x: d[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_train = pd.read_pickle('LABELS_TRAIN.df')\n",
    "idx_test = pd.read_pickle('LABELS_TEST.df')\n",
    "\n",
    "label_train = idx_train['relevance']\n",
    "idx_train.drop('relevance', axis=1, inplace=True)\n",
    "idx_test.drop('relevance', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries = pd.read_pickle('FEATURES_WITH_TEXT_1')\n",
    "df_train = idx_train.merge(queries, left_index=True, right_index=True)\n",
    "df_test = idx_test.merge(queries, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn import pipeline, model_selection\n",
    "from sklearn import pipeline, grid_search\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "def fmse(ground_truth, predictions):\n",
    "    return mean_squared_error(ground_truth, predictions)\n",
    "\n",
    "#RMSE  = make_scorer(fmse, greater_is_better=False)\n",
    "RMSE  = make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "tsvd = TruncatedSVD(n_components=10, random_state = 2016)\n",
    "randomForestRegressor = RandomForestRegressor(n_estimators = 500, min_samples_leaf=3, n_jobs = -1, random_state = 5017, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query\tproduct_uid\tproduct_title\tproduct_description\tbrand\tattrs\n",
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, hd_searches):\n",
    "        d_col_drops=['query', 'product_uid', 'product_title','product_description', 'brand', 'attrs']\n",
    "        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n",
    "        return hd_searches\n",
    "\n",
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = pipeline.Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "                    transformer_list = [\n",
    "                        ('cst',  cust_regression_vals()),  \n",
    "                    \n",
    "#                         ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='query')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n",
    "#                         ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),\n",
    "#                         ('txt3', pipeline.Pipeline([('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf), ('tsvd3', tsvd)])),\n",
    "#                         ('txt4', pipeline.Pipeline([('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf), ('tsvd4', tsvd)])),\n",
    "#                         ('txt5', pipeline.Pipeline([('s5', cust_txt_col(key='attrs')), ('tfidf5', tfidf), ('tsvd5', tsvd)]))\n",
    "                    \n",
    "#                         ('txt1', pipeline.Pipeline([ ('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf)  ])),\n",
    "#                         ('txt2', pipeline.Pipeline([ ('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf)  ])),\n",
    "#                         ('txt3', pipeline.Pipeline([ ('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf) ])),\n",
    "#                         ('txt4', pipeline.Pipeline([ ('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf) ]))\n",
    "                    \n",
    "#                         ('brandf', pipeline.Pipeline([ ('s5', cust_txt_col(key='brand_feature')), ('ohenc', ohenc)  ])),\n",
    "                        ],\n",
    "                    transformer_weights = {\n",
    "                        'cst': 1.0,\n",
    "#                         'txt1': 0.5,\n",
    "#                         'txt2': 0.25,\n",
    "#                         'txt3': 0.5,\n",
    "#                         'txt4': 0.5,\n",
    "#                         'txt5': 0.5\n",
    "#                         'brandf': 1.0\n",
    "                        },\n",
    "                n_jobs = -1\n",
    "                ))\n",
    "#         , \n",
    "#         ('rfr', randomForestRegressor)\n",
    "    ])\n",
    "\n",
    "#clf.set_params(rfr__max_features=10, rfr__max_depth=20)\n",
    "#clf.fit(X_train, y_train)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = clf.fit_transform(df_train)\n",
    "np.save('FEATURES_1z_TRAIN', a)\n",
    "b = clf.transform(df_test)\n",
    "np.save('FEATURES_1z_TEST', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "param_grid = {'rfr__max_features': [2], 'rfr__max_depth': [30]}\n",
    "model = grid_search.GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 5, verbose = 20, scoring=RMSE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Training: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALL_TEXT = []\n",
    "df_train['product_title'].map(lambda x: ALL_TEXT.append(x))\n",
    "df_train['product_description'].map(lambda x: ALL_TEXT.append(x))\n",
    "df_train['attrs'].map(lambda x: ALL_TEXT.append(x))\n",
    "\n",
    "tfidf_common = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', min_df=20)\n",
    "tfidf_common.fit(ALL_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p1 = tfidf_common.transform(df_train['query'])\n",
    "p2 = tfidf_common.transform(df_train['product_title'])\n",
    "p3 = tfidf_common.transform(df_train['product_description'])\n",
    "p4 = tfidf_common.transform(df_train['brand'])\n",
    "p5 = tfidf_common.transform(df_train['attrs'])\n",
    "\n",
    "p1t = tfidf_common.transform(df_test['query'])\n",
    "p2t = tfidf_common.transform(df_test['product_title'])\n",
    "p3t = tfidf_common.transform(df_test['product_description'])\n",
    "p4t = tfidf_common.transform(df_test['brand'])\n",
    "p5t = tfidf_common.transform(df_test['attrs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "descrs = sparse.vstack([p3, p3t, p2, p2t, p5, p5t])\n",
    "tsvd_common = TruncatedSVD(n_components=100, random_state = 2016)\n",
    "tsvd_common.fit(descrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp1 = tsvd_common.transform(p1)\n",
    "tp2 = tsvd_common.transform(p2)\n",
    "tp3 = tsvd_common.transform(p3)\n",
    "tp4 = tsvd_common.transform(p4)\n",
    "tp5 = tsvd_common.transform(p5)\n",
    "\n",
    "tp1t = tsvd_common.transform(p1t)\n",
    "tp2t = tsvd_common.transform(p2t)\n",
    "tp3t = tsvd_common.transform(p3t)\n",
    "tp4t = tsvd_common.transform(p4t)\n",
    "tp5t = tsvd_common.transform(p5t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN = np.hstack([tp1, tp2, tp3, tp4, tp5, df_train.drop(['query', 'product_uid', 'product_title','product_description', 'brand', 'attrs'], axis=1)])\n",
    "TEST = np.hstack([tp1t, tp2t, tp3t, tp4t, tp5t, df_test.drop(['query', 'product_uid', 'product_title','product_description', 'brand', 'attrs'], axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN = np.hstack([p1, p2, p3, p4, p5, df_train.drop(['query', 'product_uid', 'product_title','product_description', 'brand', 'attrs'], axis=1)])\n",
    "TEST = np.hstack([p1t, p2t, p3t, p4t, p5t, df_test.drop(['query', 'product_uid', 'product_title','product_description', 'brand', 'attrs'], axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.hstack([p1, p2, p3, p4, p5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p1.shape, p2.shape, df_train.shape, p1t.shape, p2t.shape, df_test.shape, p1.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN.shape, TEST.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('FEATURES_1f_TRAIN', TRAIN)\n",
    "np.save('FEATURES_1f_TEST', TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_train.iloc[[0,3,4,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(label_train, n_folds=3, shuffle=True, random_state=117)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = a[train_index], a[test_index]\n",
    "    Y_train, Y_test = label_train.iloc[train_index], label_train.iloc[test_index]\n",
    "    model = randomForestRegressor.fit(X_train, Y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    err = fmse(yhat, Y_test)\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(a, label=label_train)\n",
    "dtrain.save_binary(\"train_f1z.buffer\")\n",
    "\n",
    "dtest = xgb.DMatrix(b)\n",
    "dtest.save_binary(\"test_f1z.buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.load('FEATURES_1z_TRAIN.npy')\n",
    "b = np.load('FEATURES_1z_TEST.npy')\n",
    "\n",
    "dtrain = xgb.DMatrix(a, label=label_train)\n",
    "dtrain.save_binary(\"train_f1z.buffer\")\n",
    "\n",
    "dtest = xgb.DMatrix(b)\n",
    "dtest.save_binary(\"test_f1z.buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(\"train_f1d.buffer\")\n",
    "dtest = xgb.DMatrix(\"test_f1d.buffer\")\n",
    "a = np.load('FEATURES_1z_TRAIN.npy')\n",
    "b = np.load('FEATURES_1z_TEST.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(a, dtrain.get_label(), test_size=0.15, random_state=1513)\n",
    "gX_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "gX_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "evallist  = [(gX_train,'train'),(gX_test,'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  9 -> 778 0.4673\n",
    "# 10 0.8 0.9 -> 587 0.4653    620 (10->15%) 0.4684\n",
    "# 11 -> 630 0.4655\n",
    "# 12 \n",
    "# 13 -> 552 0.4654\n",
    "# 15 -> 500 0.4666\n",
    "param = {'max_depth':7, \n",
    "         'eta':0.05, # 'objective':'reg:linear',\n",
    "         'eval_metric':'rmse', #'maximize': False,\n",
    "         'colsample_bytree':0.8, #7\n",
    "         'subsample':0.9,  #8\n",
    "         'min_child_weight': 4.0,\n",
    "         'nthread':16,\n",
    "         'silent': True\n",
    "        }\n",
    "num_round = 10000\n",
    "bst = xgb.train( param, gX_train, num_round, [(gX_train,'train'),(gX_test,'test')], early_stopping_rounds=30, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'max_depth':7, \n",
    "         'eta':0.05, # 'objective':'reg:linear',\n",
    "         'eval_metric':'rmse', #'maximize': False,\n",
    "         'colsample_bytree':0.8, #7\n",
    "         'subsample':0.9,  #8\n",
    "         'min_child_weight': 4.0,\n",
    "         'nthread':16,\n",
    "         'silent': True\n",
    "        }\n",
    "num_round = 800\n",
    "bst = xgb.train( param, dtrain, num_round, [(gX_test,'test')], verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge (alpha = .1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "y_hat = np.minimum(np.maximum(y_hat, 1.0), 3.0)\n",
    "fmean_squared_error(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = bst.predict(dtest)\n",
    "y_pred_bounded = np.minimum(np.maximum(y_pred, 1.0), 3.0)\n",
    "idx_test['relevance'] = y_pred_bounded\n",
    "idx_test.to_csv('submission_xgboost_spells_0332_0335.csv')\n",
    "\n",
    "y_pred_t = bst.predict(dtrain)\n",
    "y_pred_t_bounded = np.minimum(np.maximum(y_pred_t, 1.0), 3.0)\n",
    "pd.DataFrame({\"relevance\": y_pred_t_bounded}, index=idx_train.index).to_csv('submission_xgboost_spells_0332_0335__train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## COPY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(\"good_feats/train.buffer\")\n",
    "dtest = xgb.DMatrix(\"good_feats/test.buffer\")\n",
    "evallist  = [(dtrain,'train')]\n",
    "\n",
    "a = np.load('good_feats/train_data.npy')\n",
    "b = np.load('good_feats/test_data.npy')\n",
    "a_brand = np.load('good_feats/features_brand_01_train.npy')\n",
    "b_brand = np.load('good_feats/features_brand_01_test.npy')\n",
    "a_other = np.load('FEATURES_1d_TRAIN.npy')\n",
    "b_other = np.load('FEATURES_1d_TEST.npy')\n",
    "a = np.hstack((a, a_brand, a_other))\n",
    "b = np.hstack((b, b_brand, b_other))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(a, dtrain.get_label(), test_size=0.10, random_state=147)\n",
    "gX_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "gX_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "evallist  = [(gX_train,'train'),(gX_test,'test')]\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "param = {'max_depth':12, \n",
    "         'eta':0.01, # 'objective':'reg:linear',\n",
    "         'eval_metric':'rmse', #'maximize': False,\n",
    "         'colsample_bytree':0.8, #7\n",
    "         'subsample':0.9,  #8\n",
    "         'min_child_weight': 3.0,\n",
    "         'nthread':16,\n",
    "         'silent': True\n",
    "        }\n",
    "num_round = 5000\n",
    "bst = xgb.train( param, gX_train, num_round, [(gX_train,'train'),(gX_test,'test')], early_stopping_rounds=15, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ggXtrain = xgb.DMatrix(data=a, label=dtrain.get_label())\n",
    "ggXtest = xgb.DMatrix(data=b, label=dtest.get_label())\n",
    "\n",
    "num_round = 500\n",
    "bst = xgb.train( param, gX_train, num_round, [(gX_test,'test')], early_stopping_rounds=15, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ggXtest = xgb.DMatrix(data=b, label=dtest.get_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat = bst.predict(ggXtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_bounded = np.minimum(np.maximum(yhat, 1.0), 3.0)\n",
    "idx_test['relevance'] = y_pred_bounded\n",
    "idx_test.to_csv('submission_xgboost_3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
