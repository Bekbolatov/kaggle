{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn import pipeline, model_selection\n",
    "from sklearn import pipeline, grid_search\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "#from nltk.metrics import edit_distance\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "#from nltk.stem.snowball import SnowballStemmer #0.003 improvement but takes twice as long as PorterStemmer\n",
    "#stemmer = SnowballStemmer('english')\n",
    "import re\n",
    "#import enchant\n",
    "import random\n",
    "random.seed(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOC = '/Users/rbekbolatov/data/kaggle/homedepot/'\n",
    "df_train = pd.read_csv(LOC + 'train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv(LOC + 'test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv(LOC + 'product_descriptions.csv')\n",
    "df_attr = pd.read_csv(LOC + 'attributes.csv')\n",
    "\n",
    "df_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n",
    "num_train = df_train.shape[0]\n",
    "\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4x2\n",
    "XBY = \"xby\"\n",
    "pattern_xby_d = re.compile(r\"(x[0-9])\")\n",
    "pattern_d_xby = re.compile(r\"([0-9])x\")\n",
    "\n",
    "# units\n",
    "pattern_inch = re.compile(r\"([0-9]+)( *)(inches|inch|in|')\\.?\")\n",
    "pattern_foot = re.compile(r\"([0-9]+)( *)(foot|feet|ft|''|\\\")\\.?\")\n",
    "pattern_pound = re.compile(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\")\n",
    "pattern_sqft = re.compile(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\")\n",
    "pattern_gallons = re.compile(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\")\n",
    "pattern_oz = re.compile(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\")\n",
    "pattern_cm = re.compile(r\"([0-9]+)( *)(centimeters|cm)\\.?\")\n",
    "pattern_mm = re.compile(r\"([0-9]+)( *)(milimeters|mm)\\.?\")\n",
    "pattern_deg = re.compile(r\"([0-9]+)( *)(degrees|degree)\\.?\")\n",
    "pattern_volt = re.compile(r\"([0-9]+)( *)(volts|volt)\\.?\")\n",
    "pattern_watt = re.compile(r\"([0-9]+)( *)(watts|watt)\\.?\")\n",
    "pattern_amp = re.compile(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\")\n",
    "\n",
    "# split\n",
    "pattern_split = re.compile('[^0-9a-z]')\n",
    "\n",
    "def str_stem(s): \n",
    "    if isinstance(s, str) or isinstance(s, unicode):\n",
    "        s = s.lower().strip()\n",
    "        \n",
    "        # 4ft x 2ft\n",
    "        s = s.replace(\" x \",\" \" + XBY + \" \")\n",
    "        s = s.replace(\"*\",\" \" + XBY + \" \")        \n",
    "        s = s.replace(\" by \",\" \" + XBY)\n",
    "        s = pattern_xby_d.sub(\" \" + XBY + \" \\1\", s)\n",
    "        s = pattern_d_xby.sub(\"\\1 \" + XBY + \" \", s)\n",
    "        \n",
    "        # units\n",
    "        s = pattern_inch.sub(r\"\\1 inch \", s)\n",
    "        s = pattern_foot.sub(r\"\\1 foot \", s)\n",
    "        s = pattern_pound.sub(r\"\\1 pound \", s)\n",
    "        s = pattern_sqft.sub(r\"\\1 sqft \", s)\n",
    "        s = pattern_gallons.sub(r\"\\1 gal \", s)\n",
    "        s = pattern_oz.sub(r\"\\1 oz \", s)\n",
    "        s = pattern_cm.sub(r\"\\1 cm \", s)\n",
    "        s = pattern_mm.sub(r\"\\1 mm \", s)\n",
    "        s = pattern_deg.sub(r\"\\1 deg \", s)\n",
    "        s = pattern_volt.sub(r\"\\1 volt \", s)\n",
    "        s = pattern_watt.sub(r\"\\1 watt \", s)\n",
    "        s = pattern_amp.sub(r\"\\1 amp \", s)\n",
    "        \n",
    "        # some by hand\n",
    "        s = s.replace(\"whirpool\",\"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolga\", \"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolstainless\",\"whirlpool stainless\")\n",
    "        \n",
    "        s = ' '.join([x for x in pattern_split.split(s) if x])\n",
    "        return s\n",
    "    else:\n",
    "        return 'null'\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    words, cnt = str1.split(), 0\n",
    "    for word in words:\n",
    "        if str2.find(word)>=0:\n",
    "            cnt+=1\n",
    "    return cnt\n",
    "\n",
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "    return cnt\n",
    "\n",
    "# def fmean_squared_error(ground_truth, predictions):\n",
    "#     fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "#     return fmean_squared_error_\n",
    "\n",
    "# RMSE  = make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, hd_searches):\n",
    "        d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand']\n",
    "        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n",
    "        return hd_searches\n",
    "\n",
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].apply(str)\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "RMSE  = make_scorer(fmean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#comment out the lines below use df_all.csv for further grid search testing\n",
    "#if adding features consider any drops on the 'cust_regression_vals' class\n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:str_stem(x))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:str_stem(x))\n",
    "df_all['product_description'] = df_all['product_description'].map(lambda x:str_stem(x))\n",
    "df_all['brand'] = df_all['brand'].map(lambda x:str_stem(x))\n",
    "df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_title'] = df_all['product_title'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_description'] = df_all['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_brand'] = df_all['brand'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['product_info'] = df_all['search_term']+\"\\t\"+df_all['product_title'] +\"\\t\"+df_all['product_description']\n",
    "df_all['query_in_title'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "df_all['query_in_description'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))\n",
    "df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "df_all['ratio_title'] = df_all['word_in_title']/df_all['len_of_query']\n",
    "df_all['ratio_description'] = df_all['word_in_description']/df_all['len_of_query']\n",
    "df_all['attr'] = df_all['search_term']+\"\\t\"+df_all['brand']\n",
    "df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_all['ratio_brand'] = df_all['word_in_brand']/df_all['len_of_brand']\n",
    "df_brand = pd.unique(df_all.brand.ravel())\n",
    "d={}\n",
    "i = 1\n",
    "for s in df_brand:\n",
    "    d[s]=i\n",
    "    i+=1\n",
    "df_all['brand_feature'] = df_all['brand'].map(lambda x:d[x])\n",
    "df_all['search_term_feature'] = df_all['search_term'].map(lambda x:len(x))\n",
    "#df_all.to_csv('df_all.csv')\n",
    "#df_all = pd.read_csv('df_all.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "df_train = df_all.iloc[:num_train]\n",
    "df_test = df_all.iloc[num_train:]\n",
    "id_test = df_test['id']\n",
    "y_train = df_train['relevance'].values\n",
    "X_train = df_train[:]\n",
    "X_test = df_test[:]\n",
    "print(\"--- Features Set: %s minutes ---\" % round(((time.time() - start_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rfr = RandomForestRegressor(n_estimators = 500, n_jobs = -1, random_state = 2016, verbose = 1)\n",
    "rfr = RandomForestRegressor(n_estimators = 500, random_state = 2016, verbose = 1)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "tsvd = TruncatedSVD(n_components=10, random_state = 2016)\n",
    "clf = pipeline.Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "                    transformer_list = [\n",
    "                        ('cst',  cust_regression_vals()),  \n",
    "                        ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n",
    "                        ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),\n",
    "                        ('txt3', pipeline.Pipeline([('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf), ('tsvd3', tsvd)])),\n",
    "                        ('txt4', pipeline.Pipeline([('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf), ('tsvd4', tsvd)]))\n",
    "                        ],\n",
    "                    transformer_weights = {\n",
    "                        'cst': 1.0,\n",
    "                        'txt1': 0.5,\n",
    "                        'txt2': 0.25,\n",
    "                        'txt3': 0.0,\n",
    "                        'txt4': 0.5\n",
    "                        },\n",
    "                n_jobs = -1\n",
    "                )), \n",
    "        ('rfr', rfr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.set_params(rfr__max_features=10, rfr__max_depth=20)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "param_grid = {'rfr__max_features': [10], 'rfr__max_depth': [20]}\n",
    "model = grid_search.GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 2, verbose = 20, scoring=RMSE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)\n",
    "print(\"--- Training & Testing: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
