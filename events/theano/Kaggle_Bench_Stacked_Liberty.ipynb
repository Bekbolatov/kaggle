{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 17 23:50:55 2015\n",
    "\n",
    "@author: Justfor <justformaus@gmx.de>\n",
    "\n",
    "Purpose: This script tries to implement a technique called stacking/stacked generalization.\n",
    "I made this a runnable script available because I found that there isn't really any\n",
    "readable code that demonstrates this technique. \n",
    "======================================================================================================\n",
    "Summary:\n",
    "\n",
    "Just to test an implementation of stacking/stacked generalization.\n",
    "Using a cross-validated ExtraTrees Regressor, Random forest and Gradient Boosting Regressor.\n",
    "Leaderboard: 0.361412\n",
    "Improvements should be very easy.\n",
    "\n",
    "This code is heavily inspired from the classification code shared by Emanuele (https://github.com/emanuele)\n",
    "and by Eric Chio \"log0\" <im.ckieric@gmail.com>,  but I have made it for regression, cleaned it up to\n",
    " make it available for easy download and execution.\n",
    "\n",
    "======================================================================================================\n",
    "Methodology:\n",
    "\n",
    "Three classifiers (ExtraTreesRegressor, RandomForestRegressor and a GradientBoostingRegressor)\n",
    "are built to be stacked by a RidgeCVRegressor in the end.\n",
    "\n",
    "Some terminologies first, since everyone has their own, I'll define mine to be clear:\n",
    "- DEV SET, this is to be split into the training and validation data. It will be cross-validated.\n",
    "- TEST SET, this is the unseen data to validate the generalization error of our final classifier. This\n",
    "set will never be used to train.\n",
    "When DEVELOPMENT is set True, then cross validation and evaluation takes place.\n",
    "Otherwise (DEVELOPMENT=False) a submission file is generated.\n",
    "\n",
    "======================================================================================================\n",
    "Data Set Information:\n",
    "Kaggle Competition LMP Property Inspection\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from datetime import datetime\n",
    "                                 \n",
    "# Source of good version: https://www.kaggle.com/c/ClaimPredictionChallenge/forums/t/703/code-to-calculate-normalizedgini    \n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "     assert( len(actual) == len(pred) )\n",
    "     all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "     all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "     totalLosses = all[:,0].sum()\n",
    "     giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "     giniSum -= (len(actual) + 1) / 2.\n",
    "     return giniSum / len(actual)\n",
    " \n",
    "def normalized_gini(a, p):\n",
    "     return gini(a, p) / gini(a, a)\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    # load train data\n",
    "    train    = pd.read_csv('../input/train.csv')\n",
    "    test     = pd.read_csv('../input/test.csv')\n",
    "    \n",
    "    labels   = train.Hazard   \n",
    "    test_ind = test.ix[:,'Id']\n",
    "    train.drop('Hazard', axis=1, inplace=True)\n",
    "    train_ind = train.ix[:,'Id']\n",
    "    train.drop('Id', axis=1, inplace=True)\n",
    "    test.drop('Id', axis=1, inplace=True)\n",
    "    Dropcols = ['T2_V10', 'T2_V7','T1_V13', 'T1_V10']\n",
    "    train.drop( Dropcols, axis = 1, inplace=True )\n",
    "    test.drop( Dropcols, axis = 1, inplace=True )\n",
    "    \n",
    "    catCols=['T1_V4', 'T1_V5','T1_V6', 'T1_V7', 'T1_V8', 'T1_V9', 'T1_V11', 'T1_V12', 'T1_V15',\n",
    "     'T1_V16', 'T1_V17', 'T2_V3', 'T2_V5', 'T2_V11', 'T2_V12', 'T2_V13']\n",
    "  \n",
    "    trainNumX=train.drop(catCols, axis=1)\n",
    "    trainCatVecX = pd.get_dummies(train[catCols])   \n",
    "    trainX = np.hstack((trainCatVecX,trainNumX))\n",
    "\n",
    "    testNumX=test.drop(catCols, axis=1)\n",
    "    testCatVecX = pd.get_dummies(test[catCols])  \n",
    "    testX = np.hstack((testCatVecX,testNumX))\n",
    " \n",
    "    return trainX, labels, train_ind, testX, test_ind\n",
    "     \n",
    "if __name__ == '__main__':    \n",
    "    DEVELOP = False\n",
    "\n",
    "    SEED = 42   \n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    X, Y, idx, testX, testidx = prepare_data()\n",
    "\n",
    "    print (\"Preparing models.\")\n",
    "  \n",
    "    if (DEVELOP==True):\n",
    "        # The DEV SET will be used for all training and validation purposes\n",
    "        # The TEST SET will never be used for training, it is the unseen set.\n",
    "        dev_cutoff = int(round(len(Y) * 4/5))\n",
    "        X_dev = X[:dev_cutoff]\n",
    "        Y_dev = Y[:dev_cutoff]\n",
    "        X_test = X[dev_cutoff:]\n",
    "        Y_test = Y[dev_cutoff:]              \n",
    "    else:    # else submit    \n",
    "        X_dev = X\n",
    "        Y_dev = Y\n",
    "        X_test = testX\n",
    "        \n",
    "    n_trees = 30\n",
    "    n_folds = 5\n",
    "  \n",
    "    # Our level 0 classifiers\n",
    "    clfs = [\n",
    "        ExtraTreesRegressor(n_estimators = n_trees *2),\n",
    "        RandomForestRegressor(n_estimators = n_trees),\n",
    "        GradientBoostingRegressor(n_estimators = n_trees)\n",
    "    ]\n",
    "\n",
    "    # Ready for cross validation\n",
    "    skf = KFold(n=X_dev.shape[0], n_folds=n_folds)\n",
    "        \n",
    "    # Pre-allocate the data\n",
    "    blend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiers\n",
    "    blend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers\n",
    "\n",
    "    print (\"Calculating pre-blending values.\")\n",
    "    start_time = datetime.now()\n",
    "  \n",
    "    cv_results = np.zeros((len(clfs), len(skf)))  # Number of classifiers x Number of folds\n",
    "        \n",
    "    # For each classifier, we train the number of fold times (=len(skf))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print ('\\nTraining classifier [%s]%s' % (j, clf))\n",
    "        blend_test_j = np.zeros((X_test.shape[0], len(skf))) # Number of testing data x Number of folds , we will take the mean of the predictions later\n",
    "        for i, (train_index, cv_index) in enumerate(skf):\n",
    "            #print ('Fold [%s]' % (i))\n",
    "            \n",
    "            # This is the training and validation set\n",
    "            X_train = X_dev[train_index]\n",
    "            Y_train = Y_dev[train_index]\n",
    "            X_cv = X_dev[cv_index]\n",
    "            Y_cv = Y_dev[cv_index]\n",
    "            \n",
    "            #print(\"fit\")\n",
    "            clf.fit(X_train, Y_train)\n",
    "            \n",
    "            #print(\"blend\")\n",
    "            # This output will be the basis for our blended classifier to train against,\n",
    "            # which is also the output of our classifiers\n",
    "            one_result = clf.predict(X_cv)\n",
    "            blend_train[cv_index, j] = one_result\n",
    "            score = normalized_gini(Y_cv, blend_train[cv_index, j])\n",
    "            cv_results[j,i] = score\n",
    "            score_mse = metrics.mean_absolute_error(Y_cv, one_result)    \n",
    "            print ('Fold [%s] norm. Gini = %0.5f, MSE = %0.5f' % (i, score, score_mse)) \n",
    "            blend_test_j[:, i] = clf.predict(X_test)       \n",
    "        # Take the mean of the predictions of the cross validation set\n",
    "        blend_test[:, j] = blend_test_j.mean(1)      \n",
    "        print ('Clf_%d Mean norm. Gini = %0.5f (%0.5f)' % (j, cv_results[j,].mean(), cv_results[j,].std()))\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    time_taken = end_time - start_time\n",
    "    print (\"Time taken for pre-blending calculations: \", time_taken)\n",
    "\n",
    "    print (\"CV-Results\", cv_results)\n",
    "    \n",
    "    # Start blending!    \n",
    "    print (\"Blending models.\")\n",
    "\n",
    "    alphas = [0.0001, 0.005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "    \n",
    "    bclf = RidgeCV(alphas=alphas, normalize=True, cv=5)\n",
    "    bclf.fit(blend_train, Y_dev)       \n",
    "    print (\"Ridge Best alpha = \", bclf.alpha_)\n",
    "   \n",
    "    # Predict now\n",
    "    Y_test_predict = bclf.predict(blend_test)\n",
    "    \n",
    "    if (DEVELOP):\n",
    "            score1 = metrics.mean_absolute_error(Y_test, Y_test_predict)\n",
    "            score = normalized_gini(Y_test, Y_test_predict)\n",
    "            print ('Ridge MSE = %s normalized Gini = %s' % (score1, score))\n",
    "    else: # Submit! and generate solution\n",
    "        score = cv_results.mean()      \n",
    "        print ('Avg. CV-Score = %s' % (score))\n",
    "        #generate solution\n",
    "        submission = pd.DataFrame({\"Id\": testidx, \"Hazard\": Y_test_predict})\n",
    "        submission = submission.set_index('Id')\n",
    "        submission.to_csv(\"bench_gen_stacking.csv\") \n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
